{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ćwiczeniu 6 należy zbudować sieć wykorzystującą warstwy CNN i operator max\n",
    "pooling. Warstwa CNN dostępna jest w torch.nn. Conv2d:\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "Warstwa przyjmuje 4-wymiarowe wejścia (batch, kanały obrazka, wysokość,\n",
    "szerokość). Dla obrazków czarno-białych liczba kanałów wejścia będzie równa 1,\n",
    "liczba kanałów wyjściowych powinna być większa. Po przejściu przez jedną lub więcej\n",
    "warstw konwolucyjnych, wyjście trzeba spłaszczyć i podać do warstwy liniowej, dalej\n",
    "skorzystać ze standardowej dla klasyfikacji funkcji kosztu.\n",
    "Do implementacji warstwy liniowej można skorzystać z LazyLinear, która\n",
    "automatycznie ustala wymiarowość przy pierwszym przejściu danych.\n",
    "Do przebadania będzie\n",
    "• Liczba kanałów wyjściowych warstwy konwolucyjnej\n",
    "• Rozmiar filtra warstwy konwolucyjnej\n",
    "• Rozmiar okna poolingu\n",
    "• Zaburzenia danych: dane można zaburzyć dodając do wejściowego batcha\n",
    "batch o tych samych wymiarach, wygenerowany jako szum gaussowski o\n",
    "różnych odchyleniach. Przebadać scenariusze: szum dodany w danych\n",
    "testowych vs szum dodany zarówno w testowych, jak i treningowych.\n",
    "Ćwiczenie oceniane jest w skali 0-10 pkt, na jego wykonanie są 2 tygodnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score\n",
    "from torchvision.transforms import GaussianBlur\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import LazyLinear\n",
    "\n",
    "default_batch_size = 100\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "transform_blur = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    GaussianBlur(kernel_size=9, sigma=(1.5, 5.0))\n",
    "])\n",
    "\n",
    "#how to get train and test data\n",
    "train_data = datasets.FashionMNIST('path', download=True, train=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('path', download=True, train=False, transform=transform)\n",
    "\n",
    "# use only 5% of train data\n",
    "train_indices = list(range(len(train_data)))\n",
    "random.shuffle(train_indices)\n",
    "train_indices_5_percent = train_indices[:len(train_indices)//20]\n",
    "\n",
    "test_indices = list(range(len(test_data)))\n",
    "random.shuffle(test_indices)\n",
    "test_indices_5_percent = test_indices[:len(test_indices)//20]\n",
    "# dont use subset for now\n",
    "train_data_5_percent = Subset(train_data, train_indices_5_percent)\n",
    "test_data_5_percent = Subset(test_data, test_indices_5_percent)\n",
    "\n",
    "print('100%: ',len(train_data))\n",
    "print('100%: ',len(test_data))\n",
    "print('5%: ',len(train_data_5_percent))\n",
    "print('5%: ',len(test_data_5_percent))\n",
    "# print out rest of configuration\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data_5_percent, batch_size=default_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data_5_percent, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "#shoow first input shape    \n",
    "print(next(iter(train_loader))[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, criterion, data_loader, test_loader, epochs):\n",
    "    #reset model\n",
    "    model.apply(init_normal)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    start_timestamp = time.time()\n",
    "    training_loss = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 1, 28, 28))\n",
    "                labels = Variable(labels)\n",
    "                #forward pass\n",
    "                logits = model(images)\n",
    "                loss_test = criterion(logits, labels)\n",
    "                test_loss += loss_test.item()\n",
    "\n",
    "        for images, labels in data_loader:\n",
    "            images = Variable(images.view(-1, 1, 28, 28))\n",
    "            labels = Variable(labels)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss = running_loss/len(data_loader)\n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        training_loss.append(running_loss) \n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining Time (in seconds) = {(time.time()-start_timestamp):.2f}\")\n",
    "    return training_loss, test_loss_list\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    train, test = losses\n",
    "    plt.plot(train)\n",
    "    plt.plot(test)\n",
    "    plt.legend(['Train' ,'Test'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_channels, kernel_size, pool_size):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer with adjustable parameters\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, num_channels, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size, stride=2)\n",
    "        )\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            LazyLinear(num_channels * 14 * 14),\n",
    "            nn.LeakyReLU(),\n",
    "            LazyLinear(128),\n",
    "            nn.LeakyReLU(),\n",
    "            LazyLinear(10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.manual_seed(seed)\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "        if hasattr(m, 'bias'):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 1, 28, 28)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score\n",
    "from torchvision.transforms import GaussianBlur\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import LazyLinear\n",
    "\n",
    "default_batch_size = 200\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "transform_blur = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    GaussianBlur(kernel_size=9, sigma=(1.5, 5.0))\n",
    "])\n",
    "\n",
    "#how to get train and test data\n",
    "train_data = datasets.FashionMNIST('path', download=True, train=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('path', download=True, train=False, transform=transform)\n",
    "\n",
    "#train_data with blur\n",
    "train_data_noise = datasets.FashionMNIST('path', download=True, train=True, transform=transform_blur)\n",
    "test_data_noise = datasets.FashionMNIST('path', download=True, train=False, transform=transform_blur)\n",
    "\n",
    "# use only 10% of train data\n",
    "train_indices = list(range(len(train_data)))\n",
    "random.shuffle(train_indices)\n",
    "train_indices_10_percent = train_indices[:len(train_indices)//10]\n",
    "\n",
    "test_indices = list(range(len(test_data)))\n",
    "random.shuffle(test_indices)\n",
    "test_indices_10_percent = test_indices[:len(test_indices)//10]\n",
    "# dont use subset for now\n",
    "train_data_10_percent = Subset(train_data, train_indices_10_percent)\n",
    "test_data_10_percent = Subset(test_data, test_indices_10_percent)\n",
    "\n",
    "train_data_10_percent_noisy = Subset(train_data_noise, train_indices_10_percent)\n",
    "test_data_10_percent_noisy = Subset(test_data_noise, test_indices_10_percent)\n",
    "\n",
    "print('100%: ',len(train_data))\n",
    "print('100%: ',len(test_data))\n",
    "print('10%: ',len(train_data_10_percent))\n",
    "print('10%: ',len(test_data_10_percent))\n",
    "# print out rest of configuration\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=default_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "train_loader_noisy = torch.utils.data.DataLoader(train_data_10_percent_noisy, batch_size=default_batch_size, shuffle=True)\n",
    "test_loader_noisy = torch.utils.data.DataLoader(test_data_10_percent_noisy, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "#shoow first input shape\n",
    "print(next(iter(train_loader))[0].shape)\n",
    "\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    train, test = losses\n",
    "    plt.plot(train)\n",
    "    plt.plot(test)\n",
    "    plt.legend(['Train' ,'Test'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.show()\n",
    "\n",
    "#every batch take batch loss\n",
    "def train(model, criterion, data_loader, test_loader, epochs):\n",
    "    # Reset model\n",
    "    model.apply(init_normal)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    start_timestamp = time.time()\n",
    "    training_loss = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(data_loader):\n",
    "            images = Variable(images.view(-1, 1, 28, 28))\n",
    "            labels = Variable(labels)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss = loss.item()\n",
    "\n",
    "            # Print training loss for every batch\n",
    "            # print(f\"Epoch {epoch}/{epochs} - Batch {batch_idx}/{len(data_loader)} - Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "            training_loss.append(running_loss)\n",
    "\n",
    "            # Testing phase\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (images, labels) in enumerate(test_loader):\n",
    "                    images = Variable(images.view(-1, 1, 28, 28))\n",
    "                    labels = Variable(labels)\n",
    "                    logits = model(images)\n",
    "                    loss_test = criterion(logits, labels)\n",
    "                    test_loss = loss_test.item()\n",
    "\n",
    "                # Print test loss for every batch\n",
    "                # print(f\"Epoch {epoch}/{epochs} - Batch {batch_idx}/{len(test_loader)} - Test Loss: {loss_test.item():.4f}\")\n",
    "\n",
    "            test_loss_list.append(test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} - Total Train Loss: {running_loss:.4f}, Total Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining Time (in seconds) = {(time.time()-start_timestamp):.2f}\")\n",
    "    return training_loss, test_loss_list\n",
    "\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_channels, kernel_size, pool_size):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, num_channels, kernel_size=kernel_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size),\n",
    "        )\n",
    "\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            LazyLinear(120),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.LeakyReLU(),\n",
    "            LazyLinear(10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.manual_seed(seed)\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "        if hasattr(m, 'bias'):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 1, 28, 28)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 16  # You can adjust this value\n",
    "kernel_size = 3    # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "#pass a dummy tensor to the model\n",
    "#Each value is the darkness of the pixel (1 to 255)\n",
    "dummy_tensor = torch.rand(1, 1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model(dummy_tensor)\n",
    "\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "plot_loss(cnn_losses, f'cnn_model num_channels = {num_channels} kernel_size = {kernel_size} pool_size = {pool_size}')\n",
    "\n",
    "#try changinf channel size\n",
    "num_channels = 32  # You can adjust this value\n",
    "kernel_size = 3    # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model(dummy_tensor)\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "plot_loss(cnn_losses,  f'cnn_model num_channels = {num_channels} kernel_size = {kernel_size} pool_size = {pool_size}')\n",
    "\n",
    "#change filter size\n",
    "\n",
    "num_channels = 16  # You can adjust this value\n",
    "kernel_size = 9   # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model(dummy_tensor)\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "plot_loss(cnn_losses,  f'cnn_model num_channels = {num_channels} kernel_size = {kernel_size} pool_size = {pool_size}')\n",
    "\n",
    "# change pool size\n",
    "\n",
    "num_channels = 16  # You can adjust this value\n",
    "kernel_size = 3    # You can adjust this value\n",
    "pool_size = 4      # You can adjust this value\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model(dummy_tensor)\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "\n",
    "plot_loss(cnn_losses,  f'cnn_model num_channels = {num_channels} kernel_size = {kernel_size} pool_size = {pool_size}')\n",
    "\n",
    "num_channels = 16  # You can adjust this value\n",
    "kernel_size = 3    # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model(dummy_tensor)\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader_noisy,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader_noisy)\n",
    "\n",
    "plot_loss(cnn_losses,  f'noisy_test num_channels = {num_channels} kernel_size = {kernel_size} pool_size = {pool_size}')\n",
    "\n",
    "\n",
    "# change data train loader to noisy\n",
    "\n",
    "num_channels = 16  # You can adjust this value\n",
    "kernel_size = 3    # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model(dummy_tensor)\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader_noisy,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "\n",
    "plot_loss(cnn_losses,  f'noisy_all num_channels = {num_channels} kernel_size = {kernel_size} pool_size = {pool_size}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
