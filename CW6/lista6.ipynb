{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score\n",
    "from torchvision.transforms import GaussianBlur\n",
    "\n",
    "default_batch_size = 500\n",
    "default_batch_size_10 = 128\n",
    "default_batch_size_1 = 64\n",
    "\n",
    "big_batch_size = 512\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "transform_blur = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    GaussianBlur(kernel_size=9, sigma=(1.5, 5.0))\n",
    "])\n",
    "\n",
    "#how to get train and test data\n",
    "train_data = datasets.FashionMNIST('path', download=True, train=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('path', download=True, train=False, transform=transform)\n",
    "\n",
    "train_data_noise = datasets.FashionMNIST('path', download=True, train=True, transform=transform_blur)\n",
    "test_data_noise = datasets.FashionMNIST('path', download=True, train=False, transform=transform_blur)\n",
    "\n",
    "train_indices = list(range(len(train_data)))\n",
    "random.shuffle(train_indices)\n",
    "train_indices_10_percent = train_indices[:len(train_indices)//10]\n",
    "train_indices_1_percent = train_indices[:len(train_indices)//100]\n",
    "\n",
    "test_indices = list(range(len(test_data)))\n",
    "random.shuffle(test_indices)\n",
    "test_indices_10_percent = test_indices[:len(test_indices)//10]\n",
    "test_indices_1_percent = test_indices[:len(test_indices)//100]\n",
    "\n",
    "train_data_10_percent = Subset(train_data, train_indices_10_percent)\n",
    "test_data_10_percent = Subset(test_data, test_indices_10_percent)\n",
    "\n",
    "train_data_1_percent = Subset(train_data, train_indices_1_percent)\n",
    "test_data_1_percent = Subset(test_data, test_indices_1_percent)\n",
    "\n",
    "train_data_10_percent_noisy = Subset(train_data_noise, train_indices_10_percent)\n",
    "test_data_10_percent_noisy = Subset(test_data_noise, test_indices_10_percent)\n",
    "\n",
    "print('100%: ',len(train_data))\n",
    "print('100%: ',len(test_data))\n",
    "print('10%: ',len(train_data_10_percent))\n",
    "print('10%: ',len(test_data_10_percent))\n",
    "print('1%: ',len(train_data_1_percent))\n",
    "print('1%: ',len(test_data_1_percent))\n",
    "# print out rest of configuration\n",
    "print('Default batch size: ',default_batch_size)\n",
    "print('Big batch size: ',big_batch_size)\n",
    "\n",
    "#Define a dataloader to load data\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=default_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "train_loader_10 = torch.utils.data.DataLoader(train_data_10_percent, batch_size=default_batch_size_10, shuffle=True)\n",
    "test_loader_10 = torch.utils.data.DataLoader(test_data_10_percent, batch_size=default_batch_size_10, shuffle=True)\n",
    "\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_data_1_percent, batch_size=default_batch_size_1, shuffle=True)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_data_1_percent, batch_size=default_batch_size_1, shuffle=True)\n",
    "\n",
    "train_loader_big_batch = torch.utils.data.DataLoader(train_data_10_percent, batch_size=big_batch_size, shuffle=True)\n",
    "test_loader_big_batch = torch.utils.data.DataLoader(test_data_10_percent, batch_size=big_batch_size, shuffle=True)\n",
    "\n",
    "#add gausian blur to 10% of train data make a subset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_10_percent_noisy_loader = torch.utils.data.DataLoader(train_data_10_percent_noisy, batch_size=default_batch_size_10, shuffle=True)\n",
    "test_data_10_percent_noisy_loader = torch.utils.data.DataLoader(test_data_10_percent_noisy, batch_size=default_batch_size_10, shuffle=True)\n",
    "\n",
    "print('10%:' ,len(train_loader_10))\n",
    "print('10%:' ,len(test_loader_10))\n",
    "\n",
    "\n",
    "print('10% noisy: ',len(train_data_10_percent_noisy_loader))\n",
    "print('10% noisy: ',len(test_data_10_percent_noisy_loader))\n",
    "\n",
    "def train(model, criterion, data_loader, test_loader, epochs):\n",
    "    #reset model\n",
    "    model.apply(init_normal)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    start_timestamp = time.time()\n",
    "    training_loss = []\n",
    "    test_loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 1, 28, 28)\n",
    "                logits = model(images)\n",
    "                loss_test = criterion(logits, labels)\n",
    "                test_loss += loss_test.item()\n",
    "\n",
    "        for images, labels in data_loader:\n",
    "            images = images.view(-1, 1, 28, 28)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss = running_loss/len(data_loader)\n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        training_loss.append(running_loss) \n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining Time (in seconds) = {(time.time()-start_timestamp):.2f}\")\n",
    "    return training_loss, test_loss_list\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    train, test = losses\n",
    "    plt.plot(train)\n",
    "    plt.plot(test)\n",
    "    plt.legend(['Train' ,'Test'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Define the network architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),  # dim 28x28\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # dim 14x14\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),  # dim 14x14\n",
    "    nn.LeakyReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # dim 7x7\n",
    "    nn.Dropout2d(0.3),\n",
    "    # nn.BatchNorm2d(64),\n",
    "    nn.Flatten(),  # dim [1,]\n",
    "    nn.BatchNorm1d(64*7*7),\n",
    "    nn.Linear(64*7*7, 256),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "oneLayerModel_big_batch = nn.Sequential(\n",
    "                    nn.Linear(784, 16),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(16, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "oneLayerModel_less_neurons = nn.Sequential(\n",
    "                      nn.Linear(784, 8),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Linear(8, 10),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "\n",
    "#two layer model\n",
    "twoLayerModel = nn.Sequential(\n",
    "                    nn.Linear(784, 16),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(16, 8),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(8, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "twoLayerModel_big_batch = nn.Sequential(\n",
    "                    nn.Linear(784, 16),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(16, 8),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(8, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "twoLayerModel_less_neurons = nn.Sequential(\n",
    "                        nn.Linear(784, 8),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(8, 4),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(4, 10),\n",
    "                        nn.LogSoftmax(dim = 1))\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.manual_seed(seed)\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "# Initialize the weights\n",
    "model.apply(init_normal)\n",
    "oneLayerModel_less_neurons.apply(init_normal)\n",
    "oneLayerModel_big_batch.apply(init_normal)\n",
    "twoLayerModel.apply(init_normal)\n",
    "twoLayerModel_big_batch.apply(init_normal)\n",
    "twoLayerModel_less_neurons.apply(init_normal)\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss();\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 1, 28, 28)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('oneLayerModel 100%')\n",
    "# oneLayerModel_losses = train(\n",
    "#     model, \n",
    "#     criterion, \n",
    "#     train_loader, \n",
    "#     test_loader, \n",
    "#     epochs=10)\n",
    "# evaluate_model(model, test_loader)\n",
    "# plot_loss(oneLayerModel_losses, 'Losses oneLayerModel 100%')\n",
    "\n",
    "# print('oneLayerModel_less_neurons 100%')\n",
    "# oneLayerModel_less_neurons_losses = train(\n",
    "#     oneLayerModel_less_neurons, \n",
    "#     criterion, \n",
    "#     train_loader, \n",
    "#     test_loader, \n",
    "#     epochs=10)\n",
    "# evaluate_model(oneLayerModel_less_neurons, test_loader)\n",
    "# plot_loss(oneLayerModel_less_neurons_losses, 'Losses oneLayerModel_less_neurons 100%')\n",
    "\n",
    "# print('twoLayerModel 100%')\n",
    "# twoLayerModel_losses = train(\n",
    "#     twoLayerModel,\n",
    "#     criterion,\n",
    "#     train_loader,\n",
    "#     test_loader,\n",
    "#     epochs=10)\n",
    "# evaluate_model(twoLayerModel, test_loader)\n",
    "# plot_loss(twoLayerModel_losses, 'Losses twoLayerModel 100%')\n",
    "\n",
    "# print('twoLayerModel_less_neurons 100%')\n",
    "# twoLayerModel_less_neurons_losses = train(\n",
    "#     twoLayerModel_less_neurons, \n",
    "#     criterion, \n",
    "#     train_loader, \n",
    "#     test_loader, \n",
    "#     epochs=10)\n",
    "# evaluate_model(twoLayerModel_less_neurons, test_loader)\n",
    "# plot_loss(twoLayerModel_less_neurons_losses, 'Losses twoLayerModel_less_neurons 100%')\n",
    "\n",
    "# #-----------------10%-----------------\n",
    "\n",
    "print('oneLayerModel 10%')\n",
    "oneLayerModel_losses_10 = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    train_loader_10,\n",
    "    test_loader_10,\n",
    "    epochs=epochs)\n",
    "evaluate_model(model, test_loader_10)\n",
    "plot_loss(oneLayerModel_losses_10, 'Losses oneLayerModel 10%')\n",
    "\n",
    "# print('oneLayerModel_big_batch 10%')\n",
    "# oneLayerModel_losses_big_batch = train(\n",
    "#     oneLayerModel_big_batch, \n",
    "#     criterion, \n",
    "#     train_loader_big_batch, \n",
    "#     test_loader_big_batch, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(oneLayerModel_big_batch, test_loader_big_batch)\n",
    "# plot_loss(oneLayerModel_losses_big_batch, 'Losses oneLayerModel_big_batch 10%')\n",
    "\n",
    "# print('oneLayerModel_less_neurons 10%')\n",
    "# oneLayerModel_less_neurons_losses = train(\n",
    "#     oneLayerModel_less_neurons,\n",
    "#     criterion,\n",
    "#     train_loader_10,\n",
    "#     test_loader_10,\n",
    "#     epochs=epochs)\n",
    "# evaluate_model(oneLayerModel_less_neurons, test_loader_10)\n",
    "# plot_loss(oneLayerModel_less_neurons_losses, 'Losses oneLayerModel_less_neurons 10%')\n",
    "\n",
    "# print('twoLayerModel 10%')\n",
    "# twoLayerModel_losses = train(\n",
    "#     twoLayerModel, \n",
    "#     criterion, \n",
    "#     train_loader_10, \n",
    "#     test_loader_10, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(twoLayerModel, test_loader_10)\n",
    "# plot_loss(twoLayerModel_losses, 'Losses twoLayerModel 10%')\n",
    "\n",
    "# print('twoLayerModel_big_batch 10%')\n",
    "# twoLayerModel_losses_big_batch = train(\n",
    "#     twoLayerModel_big_batch, \n",
    "#     criterion, \n",
    "#     train_loader_big_batch, \n",
    "#     test_loader_big_batch, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(twoLayerModel_big_batch, test_loader_big_batch)\n",
    "\n",
    "# plot_loss(twoLayerModel_losses_big_batch, 'Losses twoLayerModel_big_batch 10%')\n",
    "\n",
    "\n",
    "# print('twoLayerModel_less_neurons 10%')\n",
    "# twoLayerModel_less_neurons_losses = train(\n",
    "#     twoLayerModel_less_neurons, \n",
    "#     criterion, \n",
    "#     train_loader_10, \n",
    "#     test_loader_10, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(twoLayerModel_less_neurons, test_loader_10)\n",
    "\n",
    "# plot_loss(twoLayerModel_less_neurons_losses, 'Losses twoLayerModel_less_neurons 10%')\n",
    "\n",
    "# #-----------------1%-----------------\n",
    "\n",
    "# print('oneLayerModel 1%')\n",
    "# oneLayerModel_losses_1 = train(\n",
    "#     oneLayerModel, \n",
    "#     criterion, \n",
    "#     train_loader_1, \n",
    "#     test_loader_1, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(oneLayerModel, test_loader_1)\n",
    "# plot_loss(oneLayerModel_losses_1, 'Losses oneLayerModel_1%')\n",
    "\n",
    "# print('twoLayerModel 1%')\n",
    "# #train model\n",
    "# twoLayerModel_losses_1 = train(\n",
    "#     twoLayerModel, \n",
    "#     criterion, \n",
    "#     train_loader_1, \n",
    "#     test_loader_1, \n",
    "#     epochs=epochs)\n",
    "\n",
    "# evaluate_model(twoLayerModel, test_loader_1)\n",
    "# plot_loss(twoLayerModel_losses_1, 'Losses twoLayerModel_1%')\n",
    "\n",
    "\n",
    "#-----------------10% noisy-----------------\n",
    "\n",
    "\n",
    "\n",
    "# print('oneLayerModel_10%_noisy_test')\n",
    "# oneLayerModel_losses_10_noisy = train(\n",
    "#     oneLayerModel, \n",
    "#     criterion, \n",
    "#     train_loader_10, \n",
    "#     test_data_10_percent_noisy_loader, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(oneLayerModel, test_data_10_percent_noisy_loader)\n",
    "# plot_loss(oneLayerModel_losses_10_noisy, 'Losses oneLayerModel_10%_noisy_test')\n",
    "\n",
    "\n",
    "\n",
    "# print('oneLayerModel_10%_noisy_test_and_train')\n",
    "# oneLayerModel_losses_10_noisy = train(\n",
    "#     oneLayerModel, \n",
    "#     criterion, \n",
    "#     train_data_10_percent_noisy_loader, \n",
    "#     test_data_10_percent_noisy_loader, \n",
    "#     epochs=epochs)\n",
    "\n",
    "# evaluate_model(oneLayerModel, test_data_10_percent_noisy_loader)\n",
    "# plot_loss(oneLayerModel_losses_10_noisy, 'Losses oneLayerModel_10%_noisy_test_and_train')\n",
    "\n",
    "# print('twoLayerModel_10%_noisy_test')\n",
    "# twoLayerModel_losses_10_noisy = train(\n",
    "#     twoLayerModel, \n",
    "#     criterion, \n",
    "#     train_loader_10, \n",
    "#     test_data_10_percent_noisy_loader, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(twoLayerModel, test_data_10_percent_noisy_loader)\n",
    "# plot_loss(twoLayerModel_losses_10_noisy, 'Losses twoLayerModel_10%_noisy_test')\n",
    "\n",
    "# print('twoLayerModel_10%_noisy_test_and_train')\n",
    "# twoLayerModel_losses_10_noisy = train(\n",
    "#     twoLayerModel, \n",
    "#     criterion, \n",
    "#     train_data_10_percent_noisy_loader, \n",
    "#     test_data_10_percent_noisy_loader, \n",
    "#     epochs=epochs)\n",
    "# evaluate_model(twoLayerModel, test_data_10_percent_noisy_loader)\n",
    "# plot_loss(twoLayerModel_losses_10_noisy, 'Losses twoLayerModel_10%_noisy_test_and_train')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W ćwiczeniu 6 należy zbudować sieć wykorzystującą warstwy CNN i operator max\n",
    "pooling. Warstwa CNN dostępna jest w torch.nn. Conv2d:\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "Warstwa przyjmuje 4-wymiarowe wejścia (batch, kanały obrazka, wysokość,\n",
    "szerokość). Dla obrazków czarno-białych liczba kanałów wejścia będzie równa 1,\n",
    "liczba kanałów wyjściowych powinna być większa. Po przejściu przez jedną lub więcej\n",
    "warstw konwolucyjnych, wyjście trzeba spłaszczyć i podać do warstwy liniowej, dalej\n",
    "skorzystać ze standardowej dla klasyfikacji funkcji kosztu.\n",
    "Do implementacji warstwy liniowej można skorzystać z LazyLinear, która\n",
    "automatycznie ustala wymiarowość przy pierwszym przejściu danych.\n",
    "Do przebadania będzie\n",
    "• Liczba kanałów wyjściowych warstwy konwolucyjnej\n",
    "• Rozmiar filtra warstwy konwolucyjnej\n",
    "• Rozmiar okna poolingu\n",
    "• Zaburzenia danych: dane można zaburzyć dodając do wejściowego batcha\n",
    "batch o tych samych wymiarach, wygenerowany jako szum gaussowski o\n",
    "różnych odchyleniach. Przebadać scenariusze: szum dodany w danych\n",
    "testowych vs szum dodany zarówno w testowych, jak i treningowych.\n",
    "Ćwiczenie oceniane jest w skali 0-10 pkt, na jego wykonanie są 2 tygodnie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score\n",
    "from torchvision.transforms import GaussianBlur\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import LazyLinear\n",
    "\n",
    "default_batch_size = 100\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "transform_blur = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    GaussianBlur(kernel_size=9, sigma=(1.5, 5.0))\n",
    "])\n",
    "\n",
    "#how to get train and test data\n",
    "train_data = datasets.FashionMNIST('path', download=True, train=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('path', download=True, train=False, transform=transform)\n",
    "\n",
    "# use only 5% of train data\n",
    "train_indices = list(range(len(train_data)))\n",
    "random.shuffle(train_indices)\n",
    "train_indices_5_percent = train_indices[:len(train_indices)//20]\n",
    "\n",
    "test_indices = list(range(len(test_data)))\n",
    "random.shuffle(test_indices)\n",
    "test_indices_5_percent = test_indices[:len(test_indices)//20]\n",
    "# dont use subset for now\n",
    "train_data_5_percent = Subset(train_data, train_indices_5_percent)\n",
    "test_data_5_percent = Subset(test_data, test_indices_5_percent)\n",
    "\n",
    "print('100%: ',len(train_data))\n",
    "print('100%: ',len(test_data))\n",
    "print('5%: ',len(train_data_5_percent))\n",
    "print('5%: ',len(test_data_5_percent))\n",
    "# print out rest of configuration\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data_5_percent, batch_size=default_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data_5_percent, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "#shoow first input shape    \n",
    "print(next(iter(train_loader))[0].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, criterion, data_loader, test_loader, epochs):\n",
    "    #reset model\n",
    "    model.apply(init_normal)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    start_timestamp = time.time()\n",
    "    training_loss = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = Variable(images.view(-1, 1, 28, 28))\n",
    "                labels = Variable(labels)\n",
    "                #forward pass\n",
    "                logits = model(images)\n",
    "                loss_test = criterion(logits, labels)\n",
    "                test_loss += loss_test.item()\n",
    "\n",
    "        for images, labels in data_loader:\n",
    "            images = Variable(images.view(-1, 1, 28, 28))\n",
    "            labels = Variable(labels)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss = running_loss/len(data_loader)\n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        training_loss.append(running_loss) \n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining Time (in seconds) = {(time.time()-start_timestamp):.2f}\")\n",
    "    return training_loss, test_loss_list\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    train, test = losses\n",
    "    plt.plot(train)\n",
    "    plt.plot(test)\n",
    "    plt.legend(['Train' ,'Test'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_channels, kernel_size, pool_size):\n",
    "        super(CustomCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layer with adjustable parameters\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, num_channels, kernel_size=kernel_size, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_size, stride=2)\n",
    "        )\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            LazyLinear(num_channels * 14 * 14),\n",
    "            nn.LeakyReLU(),\n",
    "            LazyLinear(128),\n",
    "            nn.LeakyReLU(),\n",
    "            LazyLinear(10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.fc_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        torch.manual_seed(seed)\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "        if hasattr(m, 'bias'):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(-1, 1, 28, 28)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "num_channels = 32  # You can adjust this value\n",
    "kernel_size = 3    # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "#pass a dummy tensor to the model\n",
    "#Each value is the darkness of the pixel (1 to 255)\n",
    "dummy_tensor = torch.rand(1, 1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model(dummy_tensor)\n",
    "\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "plot_loss(cnn_losses, 'Losses cnn_model 5%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try changinf channel size\n",
    "num_channels = 64  # You can adjust this value\n",
    "kernel_size = 3    # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "plot_loss(cnn_losses, 'Losses cnn_model 5%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change filter size\n",
    "\n",
    "num_channels = 32  # You can adjust this value\n",
    "kernel_size = 5    # You can adjust this value\n",
    "pool_size = 2      # You can adjust this value\n",
    "\n",
    "cnn_model = CustomCNN(num_channels, kernel_size, pool_size)\n",
    "cnn_model.apply(init_normal)\n",
    "\n",
    "print(cnn_model)\n",
    "\n",
    "# train model\n",
    "cnn_losses = train(\n",
    "    cnn_model,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(cnn_model, test_loader)\n",
    "plot_loss(cnn_losses, 'Losses cnn_model 5%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
