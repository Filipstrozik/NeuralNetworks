{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.0+cpu\n",
      "Is CUDA enabled? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należy dla jedno i dwuwarstwowej sieci neuronowej porównać wyniki i krzywe \n",
    "uczenia w zależności od:\n",
    "• Liczby neuronów w warstwie ukrytej\n",
    "• Rozmiaru batcha\n",
    "• Liczby przykładów uczących (należy wykorzystać podzbiory zbioru uczącego o \n",
    "różnych rozmiarach zamiast pełnego zbioru uczącego – sprawdzić rozmiary 1% \n",
    "danych, 10% danych)\n",
    "• Zaburzenia danych: dane można zaburzyć dodając do wejściowego batcha \n",
    "batch o tych samych wymiarach, wygenerowany jako szum gaussowski o\n",
    "różnych odchyleniach. Przebadać scenariusze: szum dodany w danych \n",
    "testowych vs szum dodany zarówno w testowych, jak i treningowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1000\n",
      "94\n",
      "16\n",
      "Epoch 10/100 - Loss: 0.5257911086082458\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#how to get train and test data\n",
    "train_data = datasets.FashionMNIST('path', download=True, train=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('path', download=True, train=False, transform=transform)\n",
    "\n",
    "\n",
    "train_indices = list(range(len(train_data)))\n",
    "random.shuffle(train_indices)\n",
    "train_indices_10_percent = train_indices[:len(train_indices)//10]\n",
    "\n",
    "test_indices = list(range(len(test_data)))\n",
    "random.shuffle(test_indices)\n",
    "test_indices_10_percent = test_indices[:len(test_indices)//10]\n",
    "\n",
    "train_data_10_percent = Subset(train_data, train_indices_10_percent)\n",
    "test_data_10_percent = Subset(test_data, test_indices_10_percent)\n",
    "\n",
    "\n",
    "print(len(train_data_10_percent))\n",
    "print(len(test_data_10_percent))\n",
    "#Define a dataloader to load data\n",
    "train_loader = torch.utils.data.DataLoader(train_data_10_percent, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data_10_percent, batch_size=64, shuffle=True)\n",
    "\n",
    "train_loader_big_batch = torch.utils.data.DataLoader(train_data_10_percent, batch_size=16, shuffle=True)\n",
    "test_loader_big_batch = torch.utils.data.DataLoader(test_data_10_percent, batch_size=16, shuffle=True)\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))    \n",
    "def train(model, criterion, optimizer, data_loader, test_loader, epochs):\n",
    "    strart_timestamp = time.time()\n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in data_loader:\n",
    "            # Flatten images\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            # Zero out the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass, get our logits\n",
    "            logits = model(images)\n",
    "            # Calculate the loss with the logits and the labels\n",
    "            loss = criterion(logits, labels)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "        training_loss.append(loss.item())\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item()}\")\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                # Flatten images\n",
    "                images = images.view(images.shape[0], -1)\n",
    "                # Forward pass, get our logits\n",
    "                logits = model(images)\n",
    "                # Calculate the loss with the logits and the labels\n",
    "                loss = criterion(logits, labels)\n",
    "            test_loss.append(loss.item())\n",
    "\n",
    "    print(f\"\\nTraining Time (in minutes) = {(time.time()-strart_timestamp)/60:.2f}\")\n",
    "    print(training_loss, test_loss)\n",
    "    return training_loss, test_loss\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    train, test = losses\n",
    "    plt.plot(train)\n",
    "    plt.plot(test)\n",
    "    plt.legend(['Train', 'Test'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.show()\n",
    "\n",
    "# Define the network architecture\n",
    "oneLayerModel = nn.Sequential(\n",
    "                    nn.Linear(784, 128),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(128, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "oneLayerModel_big_batch = nn.Sequential(\n",
    "                    nn.Linear(784, 128),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(128, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "oneLayerModel_less_neurons = nn.Sequential(\n",
    "    nn.Linear(784, 32),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Linear(32, 10),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "\n",
    "\n",
    "def init_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "oneLayerModel.apply(init_xavier)\n",
    "oneLayerModel_less_neurons.apply(init_xavier)\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss();\n",
    "\n",
    "# Define the optimizer\n",
    "oneLayerModel_optimizer = optim.Adam(oneLayerModel.parameters(), lr=0.0002)\n",
    "oneLayerModel_big_batch_optimizer = optim.Adam(oneLayerModel_big_batch.parameters(), lr=0.0002)\n",
    "oneLayerModel_less_neurons_optimizer = optim.Adam(oneLayerModel_less_neurons.parameters(), lr=0.0002)\n",
    "epochs = 100\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "oneLayerModel_losses = train(\n",
    "    oneLayerModel, \n",
    "    criterion, \n",
    "    oneLayerModel_optimizer, \n",
    "    train_loader, test_loader, \n",
    "    epochs=epochs)\n",
    "plot_loss(oneLayerModel_losses, 'Losses')\n",
    "evaluate_model(oneLayerModel, test_loader)\n",
    "\n",
    "oneLayerModel_less_neurons_losses = train(\n",
    "    oneLayerModel_less_neurons, \n",
    "    criterion, oneLayerModel_less_neurons_optimizer, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=epochs)\n",
    "plot_loss(oneLayerModel_less_neurons_losses, 'Losses')\n",
    "evaluate_model(oneLayerModel_less_neurons, test_loader)\n",
    "\n",
    "\n",
    "oneLayerModel_losses_big_batch = train(\n",
    "    oneLayerModel_big_batch, \n",
    "    criterion, \n",
    "    oneLayerModel_big_batch_optimizer, \n",
    "    train_loader_big_batch, \n",
    "    test_loader_big_batch, \n",
    "    epochs=epochs)\n",
    "plot_loss(oneLayerModel_losses_big_batch, 'Losses')\n",
    "evaluate_model(oneLayerModel_big_batch, test_loader_big_batch)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
