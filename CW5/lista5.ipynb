{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.0+cpu\n",
      "Is CUDA enabled? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należy dla jedno i dwuwarstwowej sieci neuronowej porównać wyniki i krzywe \n",
    "uczenia w zależności od:\n",
    "• Liczby neuronów w warstwie ukrytej\n",
    "• Rozmiaru batcha\n",
    "• Liczby przykładów uczących (należy wykorzystać podzbiory zbioru uczącego o \n",
    "różnych rozmiarach zamiast pełnego zbioru uczącego – sprawdzić rozmiary 1% \n",
    "danych, 10% danych)\n",
    "• Zaburzenia danych: dane można zaburzyć dodając do wejściowego batcha \n",
    "batch o tych samych wymiarach, wygenerowany jako szum gaussowski o\n",
    "różnych odchyleniach. Przebadać scenariusze: szum dodany w danych \n",
    "testowych vs szum dodany zarówno w testowych, jak i treningowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\filip\\Documents\\Sem7\\NeuralNetworks\\CW5\\lista5.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/filip/Documents/Sem7/NeuralNetworks/CW5/lista5.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/filip/Documents/Sem7/NeuralNetworks/CW5/lista5.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms, datasets\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/filip/Documents/Sem7/NeuralNetworks/CW5/lista5.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/filip/Documents/Sem7/NeuralNetworks/CW5/lista5.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Subset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[1;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py:1465\u001b[0m\n\u001b[0;32m   1463\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m library\n\u001b[0;32m   1464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 1465\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   1467\u001b[0m \u001b[39m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   1468\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mTORCH_CUDA_SANITIZER\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_meta_registrations.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decomp\u001b[39;00m \u001b[39mimport\u001b[39;00m _add_op_to_registry, global_decomposition_table, meta_table\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m OpOverload\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims\u001b[39;00m \u001b[39mimport\u001b[39;00m _elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_decomp\\__init__.py:169\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m decompositions\n\u001b[0;32m    168\u001b[0m \u001b[39m# populate the table\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_decomp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecompositions\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_refs\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# This list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_decomp\\decompositions.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, cast, Iterable, List, Optional, Tuple, Union\n\u001b[0;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mprims\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_prims\\__init__.py:33\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     check,\n\u001b[0;32m     19\u001b[0m     Dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     type_to_dtype,\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m \u001b[39mimport\u001b[39;00m backwards_not_supported\n\u001b[1;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m FakeTensor, FakeTensorMode\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moverrides\u001b[39;00m \u001b[39mimport\u001b[39;00m handle_torch_function, has_torch_function\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pytree\u001b[39;00m \u001b[39mimport\u001b[39;00m tree_flatten, tree_map, tree_unflatten\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_subclasses\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_tensor\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     DynamicOutputShapeException,\n\u001b[0;32m      5\u001b[0m     FakeTensor,\n\u001b[0;32m      6\u001b[0m     FakeTensorMode,\n\u001b[0;32m      7\u001b[0m     UnsupportedFakeTensorException,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_subclasses\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfake_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m CrossRefFakeMode\n\u001b[0;32m     12\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFakeTensor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFakeTensorMode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCrossRefFakeMode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_subclasses\\fake_tensor.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mweakref\u001b[39;00m \u001b[39mimport\u001b[39;00m ReferenceType\n\u001b[0;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_guards\u001b[39;00m \u001b[39mimport\u001b[39;00m Source\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m OpOverload\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     elementwise_dtypes,\n\u001b[0;32m     17\u001b[0m     ELEMENTWISE_TYPE_PROMOTION_KIND,\n\u001b[0;32m     18\u001b[0m     is_float_dtype,\n\u001b[0;32m     19\u001b[0m     is_integer_dtype,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_guards.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# TODO(voz): Stolen pattern, not sure why this is the case,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# but mypy complains.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msympy\u001b[39;00m  \u001b[39m# type: ignore[import]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     log\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mNo sympy found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sympy\\__init__.py:22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdel\u001b[39;00m sys\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmpmath\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSymPy now depends on mpmath as an external library. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSee https://docs.sympy.org/latest/install.html#mpmath for more information.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1140\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1080\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1504\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1476\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1612\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import random\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score\n",
    "\n",
    "default_batch_size = 500\n",
    "default_batch_size_10 = 128\n",
    "default_batch_size_1 = 64\n",
    "\n",
    "big_batch_size = 512\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#how to get train and test data\n",
    "train_data = datasets.FashionMNIST('path', download=True, train=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST('path', download=True, train=False, transform=transform)\n",
    "\n",
    "\n",
    "train_indices = list(range(len(train_data)))\n",
    "random.shuffle(train_indices)\n",
    "train_indices_10_percent = train_indices[:len(train_indices)//10]\n",
    "train_indices_1_percent = train_indices[:len(train_indices)//100]\n",
    "\n",
    "test_indices = list(range(len(test_data)))\n",
    "random.shuffle(test_indices)\n",
    "test_indices_10_percent = test_indices[:len(test_indices)//10]\n",
    "test_indices_1_percent = test_indices[:len(test_indices)//100]\n",
    "\n",
    "train_data_10_percent = Subset(train_data, train_indices_10_percent)\n",
    "test_data_10_percent = Subset(test_data, test_indices_10_percent)\n",
    "\n",
    "train_data_1_percent = Subset(train_data, train_indices_1_percent)\n",
    "test_data_1_percent = Subset(test_data, test_indices_1_percent)\n",
    "\n",
    "print('100%: ',len(train_data))\n",
    "print('100%: ',len(test_data))\n",
    "print('10%: ',len(train_data_10_percent))\n",
    "print('10%: ',len(test_data_10_percent))\n",
    "print('1%: ',len(train_data_1_percent))\n",
    "print('1%: ',len(test_data_1_percent))\n",
    "# print out rest of configuration\n",
    "print('Default batch size: ',default_batch_size)\n",
    "print('Big batch size: ',big_batch_size)\n",
    "\n",
    "#Define a dataloader to load data\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=default_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=default_batch_size, shuffle=True)\n",
    "\n",
    "train_loader_10 = torch.utils.data.DataLoader(train_data_10_percent, batch_size=default_batch_size_10, shuffle=True)\n",
    "test_loader_10 = torch.utils.data.DataLoader(test_data_10_percent, batch_size=default_batch_size_10, shuffle=True)\n",
    "\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_data_1_percent, batch_size=default_batch_size_1, shuffle=True)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_data_1_percent, batch_size=default_batch_size_1, shuffle=True)\n",
    "\n",
    "train_loader_big_batch = torch.utils.data.DataLoader(train_data_10_percent, batch_size=big_batch_size, shuffle=True)\n",
    "test_loader_big_batch = torch.utils.data.DataLoader(test_data_10_percent, batch_size=big_batch_size, shuffle=True)\n",
    "\n",
    "def train(model, criterion, data_loader, test_loader, epochs):\n",
    "    #reset model\n",
    "    model.apply(init_normal)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    start_timestamp = time.time()\n",
    "    training_loss = []\n",
    "    test_loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(images.shape[0], -1)\n",
    "                logits = model(images)\n",
    "                loss_test = criterion(logits, labels)\n",
    "                test_loss += loss_test.item()\n",
    "\n",
    "        for images, labels in data_loader:\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss = running_loss/len(data_loader)\n",
    "        test_loss = test_loss/len(test_loader)\n",
    "        training_loss.append(running_loss) \n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if (epoch) % 10 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Train Loss: {running_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining Time (in seconds) = {(time.time()-start_timestamp):.2f}\")\n",
    "    return training_loss, test_loss_list\n",
    "\n",
    "def plot_loss(losses, title):\n",
    "    train, test = losses\n",
    "    plt.plot(train)\n",
    "    plt.plot(test)\n",
    "    plt.legend(['Train' ,'Test'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(0, 3)\n",
    "    plt.show()\n",
    "\n",
    "def add_noise(data, std_dev):\n",
    "    noise = torch.randn_like(data) * std_dev\n",
    "    return data + noise\n",
    "\n",
    "\n",
    "# Define the network architecture\n",
    "oneLayerModel = nn.Sequential(\n",
    "                    nn.Linear(784, 16),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(16, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "oneLayerModel_big_batch = nn.Sequential(\n",
    "                    nn.Linear(784, 16),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(16, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "oneLayerModel_less_neurons = nn.Sequential(\n",
    "                      nn.Linear(784, 8),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Linear(8, 10),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "\n",
    "#two layer model\n",
    "twoLayerModel = nn.Sequential(\n",
    "                    nn.Linear(784, 16),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(16, 8),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(8, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "twoLayerModel_big_batch = nn.Sequential(\n",
    "                    nn.Linear(784, 16),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(16, 8),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Linear(8, 10),\n",
    "                    nn.LogSoftmax(dim = 1))\n",
    "\n",
    "twoLayerModel_less_neurons = nn.Sequential(\n",
    "                        nn.Linear(784, 8),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(8, 4),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Linear(4, 10),\n",
    "                        nn.LogSoftmax(dim = 1))\n",
    "\n",
    "\n",
    "def init_normal(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "\n",
    "# Initialize the weights\n",
    "oneLayerModel.apply(init_normal)\n",
    "oneLayerModel_less_neurons.apply(init_normal)\n",
    "oneLayerModel_big_batch.apply(init_normal)\n",
    "twoLayerModel.apply(init_normal)\n",
    "twoLayerModel_big_batch.apply(init_normal)\n",
    "twoLayerModel_less_neurons.apply(init_normal)\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss();\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_predictions.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "print('oneLayerModel 100%')\n",
    "oneLayerModel_losses = train(\n",
    "    oneLayerModel, \n",
    "    criterion, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=10)\n",
    "evaluate_model(oneLayerModel, test_loader)\n",
    "plot_loss(oneLayerModel_losses, 'Losses oneLayerModel 100%')\n",
    "\n",
    "print('oneLayerModel_less_neurons 100%')\n",
    "oneLayerModel_less_neurons_losses = train(\n",
    "    oneLayerModel_less_neurons, \n",
    "    criterion, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=10)\n",
    "evaluate_model(oneLayerModel_less_neurons, test_loader)\n",
    "plot_loss(oneLayerModel_less_neurons_losses, 'Losses oneLayerModel_less_neurons 100%')\n",
    "\n",
    "print('twoLayerModel 100%')\n",
    "twoLayerModel_losses = train(\n",
    "    twoLayerModel,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=10)\n",
    "evaluate_model(twoLayerModel, test_loader)\n",
    "plot_loss(twoLayerModel_losses, 'Losses twoLayerModel 100%')\n",
    "\n",
    "print('twoLayerModel_less_neurons 100%')\n",
    "twoLayerModel_less_neurons_losses = train(\n",
    "    twoLayerModel_less_neurons, \n",
    "    criterion, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=10)\n",
    "evaluate_model(twoLayerModel_less_neurons, test_loader)\n",
    "plot_loss(twoLayerModel_less_neurons_losses, 'Losses twoLayerModel_less_neurons 100%')\n",
    "\n",
    "#-----------------10%-----------------\n",
    "\n",
    "print('oneLayerModel 10%')\n",
    "oneLayerModel_losses_10 = train(\n",
    "    oneLayerModel,\n",
    "    criterion,\n",
    "    train_loader_10,\n",
    "    test_loader_10,\n",
    "    epochs=epochs)\n",
    "evaluate_model(oneLayerModel, test_loader_10)\n",
    "plot_loss(oneLayerModel_losses_10, 'Losses oneLayerModel 10%')\n",
    "\n",
    "print('oneLayerModel_big_batch 10%')\n",
    "oneLayerModel_losses_big_batch = train(\n",
    "    oneLayerModel_big_batch, \n",
    "    criterion, \n",
    "    train_loader_big_batch, \n",
    "    test_loader_big_batch, \n",
    "    epochs=epochs)\n",
    "evaluate_model(oneLayerModel_big_batch, test_loader_big_batch)\n",
    "plot_loss(oneLayerModel_losses_big_batch, 'Losses oneLayerModel_big_batch 10%')\n",
    "\n",
    "print('oneLayerModel_less_neurons 10%')\n",
    "oneLayerModel_less_neurons_losses = train(\n",
    "    oneLayerModel_less_neurons,\n",
    "    criterion,\n",
    "    train_loader_10,\n",
    "    test_loader_10,\n",
    "    epochs=epochs)\n",
    "evaluate_model(oneLayerModel_less_neurons, test_loader_10)\n",
    "plot_loss(oneLayerModel_less_neurons_losses, 'Losses oneLayerModel_less_neurons 10%')\n",
    "\n",
    "print('twoLayerModel 10%')\n",
    "twoLayerModel_losses = train(\n",
    "    twoLayerModel, \n",
    "    criterion, \n",
    "    train_loader_10, \n",
    "    test_loader_10, \n",
    "    epochs=epochs)\n",
    "evaluate_model(twoLayerModel, test_loader_10)\n",
    "plot_loss(twoLayerModel_losses, 'Losses twoLayerModel 10%')\n",
    "\n",
    "print('twoLayerModel_big_batch 10%')\n",
    "twoLayerModel_losses_big_batch = train(\n",
    "    twoLayerModel_big_batch, \n",
    "    criterion, \n",
    "    train_loader_big_batch, \n",
    "    test_loader_big_batch, \n",
    "    epochs=epochs)\n",
    "evaluate_model(twoLayerModel_big_batch, test_loader_big_batch)\n",
    "\n",
    "plot_loss(twoLayerModel_losses_big_batch, 'Losses twoLayerModel_big_batch 10%')\n",
    "\n",
    "\n",
    "print('twoLayerModel_less_neurons 10%')\n",
    "twoLayerModel_less_neurons_losses = train(\n",
    "    twoLayerModel_less_neurons, \n",
    "    criterion, \n",
    "    train_loader_10, \n",
    "    test_loader_10, \n",
    "    epochs=epochs)\n",
    "evaluate_model(twoLayerModel_less_neurons, test_loader_10)\n",
    "\n",
    "plot_loss(twoLayerModel_less_neurons_losses, 'Losses twoLayerModel_less_neurons 10%')\n",
    "\n",
    "#-----------------1%-----------------\n",
    "\n",
    "print('oneLayerModel 1%')\n",
    "oneLayerModel_losses_1 = train(\n",
    "    oneLayerModel, \n",
    "    criterion, \n",
    "    train_loader_1, \n",
    "    test_loader_1, \n",
    "    epochs=epochs)\n",
    "evaluate_model(oneLayerModel, test_loader_1)\n",
    "plot_loss(oneLayerModel_losses_1, 'Losses oneLayerModel_1%')\n",
    "\n",
    "print('twoLayerModel 1%')\n",
    "#train model\n",
    "twoLayerModel_losses_1 = train(\n",
    "    twoLayerModel, \n",
    "    criterion, \n",
    "    train_loader_1, \n",
    "    test_loader_1, \n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(twoLayerModel, test_loader_1)\n",
    "plot_loss(twoLayerModel_losses_1, 'Losses twoLayerModel_1%')\n",
    "\n",
    "\n",
    "#-----------------10% noisy-----------------\n",
    "\n",
    "#add noise to test data 10%\n",
    "test_data_10_percent_noisy = []\n",
    "for image, label in test_data_10_percent:\n",
    "    image = add_noise(image, 0.1)\n",
    "    test_data_10_percent_noisy.append((image, label))\n",
    "\n",
    "test_data_10_percent_noisy = torch.utils.data.DataLoader(test_data_10_percent_noisy, batch_size=default_batch_size_10, shuffle=True)\n",
    "\n",
    "print('oneLayerModel_10%_noisy_test')\n",
    "oneLayerModel_losses_10_noisy = train(\n",
    "    oneLayerModel, \n",
    "    criterion, \n",
    "    train_loader_10, \n",
    "    test_data_10_percent_noisy, \n",
    "    epochs=epochs)\n",
    "evaluate_model(oneLayerModel, test_data_10_percent_noisy)\n",
    "plot_loss(oneLayerModel_losses_10_noisy, 'Losses oneLayerModel_10%_noisy_test')\n",
    "\n",
    "train_data_10_percent_noisy = []\n",
    "for image, label in train_data_10_percent:\n",
    "    image = add_noise(image, 0.1)\n",
    "    train_data_10_percent_noisy.append((image, label))\n",
    "\n",
    "train_data_10_percent_noisy = torch.utils.data.DataLoader(train_data_10_percent_noisy, batch_size=default_batch_size_10, shuffle=True)\n",
    "\n",
    "print('oneLayerModel_10%_noisy_test_and_train')\n",
    "oneLayerModel_losses_10_noisy = train(\n",
    "    oneLayerModel, \n",
    "    criterion, \n",
    "    train_data_10_percent_noisy, \n",
    "    test_data_10_percent_noisy, \n",
    "    epochs=epochs)\n",
    "\n",
    "evaluate_model(oneLayerModel, test_loader_10)\n",
    "plot_loss(oneLayerModel_losses_10_noisy, 'Losses oneLayerModel_10%_noisy_test_and_train')\n",
    "\n",
    "print('twoLayerModel_10%_noisy_test')\n",
    "twoLayerModel_losses_10_noisy = train(\n",
    "    twoLayerModel, \n",
    "    criterion, \n",
    "    train_loader_10, \n",
    "    test_data_10_percent_noisy, \n",
    "    epochs=epochs)\n",
    "evaluate_model(twoLayerModel, test_data_10_percent_noisy)\n",
    "plot_loss(twoLayerModel_losses_10_noisy, 'Losses twoLayerModel_10%_noisy_test')\n",
    "\n",
    "print('twoLayerModel_10%_noisy_test_and_train')\n",
    "twoLayerModel_losses_10_noisy = train(\n",
    "    twoLayerModel, \n",
    "    criterion, \n",
    "    train_data_10_percent_noisy, \n",
    "    test_data_10_percent_noisy, \n",
    "    epochs=epochs)\n",
    "evaluate_model(twoLayerModel, test_loader_10)\n",
    "plot_loss(twoLayerModel_losses_10_noisy, 'Losses twoLayerModel_10%_noisy_test_and_train')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data, std_dev):\n",
    "    noise = torch.randn_like(data) * std_dev\n",
    "    return data + noise\n",
    "\n",
    "# Apply noise to training data\n",
    "train_data_noisy = add_noise(train_data, std_dev=0.1)\n",
    "\n",
    "# Apply noise to testing data\n",
    "test_data_noisy = add_noise(test_data, std_dev=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
