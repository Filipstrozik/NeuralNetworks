{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "X = heart_disease.data.features\n",
    "Y = heart_disease.data.targets\n",
    "\n",
    "# Convert target to binary\n",
    "Y = Y['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Join features and target\n",
    "X['target'] = Y\n",
    "# Remove records with missing values\n",
    "X = X[~np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, column, column_names):\n",
    "    dummies = pd.get_dummies(df[column], prefix=column)\n",
    "    column_names = [column + '_' + str(name) for name in column_names]\n",
    "    dummies.columns = column_names\n",
    "    dummies = dummies.astype('int64')\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  one_hot_encode(X, 'cp', ['typical_angina', 'atypical_angina', 'non-anginal_pain', 'asymptomatic'])\n",
    "X = one_hot_encode(X, 'thal', ['normal', 'ST-T_wave_abnormality', 'left_ventricular_hypertrophy'])\n",
    "X = one_hot_encode(X, 'slope', ['upsloping', 'flat', 'downsloping'])\n",
    "X = one_hot_encode(X, 'restecg', ['normal', 'fixed_defect', 'reversable_defect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target without missing values\n",
    "Y = X['target']\n",
    "X = X.drop('target', axis=1)\n",
    "\n",
    "# Normalize data to [0, 1] range\n",
    "X_norm = (X - X.min()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    def __init__(self, num_of_weights, stand_dev=1):\n",
    "        self.weights = np.random.normal(scale=stand_dev, size=num_of_weights)\n",
    "        self.bias = np.random.normal(scale=stand_dev)\n",
    "        self.X = None\n",
    "        self.derivative = None\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return np.exp(-np.logaddexp(0, -X))\n",
    "\n",
    "    def sigmoid_derivative(self, X):\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.X = inputs\n",
    "        return self.sigmoid(np.dot(inputs, self.weights) + self.bias)\n",
    "\n",
    "    def backward(self, error, weights_next_layer=None):\n",
    "        if weights_next_layer is not None:\n",
    "            error = error.T @ weights_next_layer\n",
    "        self.derivative = error * self.sigmoid_derivative(np.dot(self.X,self.weights) + self.bias)\n",
    "        return self.derivative\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.weights -=  learning_rate * np.dot(self.X.T, self.derivative)\n",
    "        self.bias -= learning_rate * np.sum(self.derivative)\n",
    "        self.X = None\n",
    "        self.derivative = None\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, num_of_inputs, hidden_layers, num_of_outputs, stand_dev=1.0):\n",
    "        self.layers = []\n",
    "        self.num_of_inputs = num_of_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_of_outputs = num_of_outputs\n",
    "        self.num_of_hidden_layers = len(hidden_layers)\n",
    "        self.stand_dev = stand_dev\n",
    "        \n",
    "        self.layers.append([Neuron(num_of_inputs, self.stand_dev) for _ in range(hidden_layers[0])])\n",
    "        for i in range(1, self.num_of_hidden_layers):\n",
    "            self.layers.append([Neuron(hidden_layers[i-1], self.stand_dev) for _ in range(hidden_layers[i])])\n",
    "            if self.num_of_outputs == 1:\n",
    "                self.layers.append([Neuron(hidden_layers[-1], self.stand_dev)])\n",
    "\n",
    "    def cross_entropy(self, y, y_pred):\n",
    "        return -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred), axis=1)\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_pred):\n",
    "        return -np.sum(y / y_pred - (1 - y) / (1 - y_pred), axis=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = np.array([neuron.forward(X) for neuron in layer]).T\n",
    "        return X        \n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # Iterate over the hidden layers in reverse order and calculate their gradients\n",
    "        output_layer = self.layers[-1]\n",
    "        output_error = np.array([neuron.backward(output_error) for neuron in output_layer])\n",
    "        weights_next_layer = np.array([neuron.weights for neuron in output_layer]).T\n",
    "\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            output_error = np.array([neuron.backward(output_error, weights_next_layer[index]) for index, neuron in enumerate(layer)])\n",
    "            weights_next_layer = np.array([neuron.weights for neuron in layer]).T\n",
    "        \n",
    "        # Return the final gradients\n",
    "        return output_error\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer:\n",
    "                neuron.update(learning_rate)\n",
    "\n",
    "    def fit(self, X, y, epochs, batch_size, learning_rate):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Jeśli batch_size jest większy niż liczba próbek, to ustawiamy go na liczbę próbek -> zwyczajnie uczymy na całym zbiorze GD\n",
    "        if batch_size > n_samples:\n",
    "            batch_size = n_samples\n",
    "        cost_list = []\n",
    "        for epoch in range(epochs):\n",
    "            random_order = np.random.permutation(n_samples)\n",
    "            X_shuffled = X.values[random_order]\n",
    "            y_shuffled = y.values[random_order]\n",
    "            # lista kosztów w paczkach dla każdej epoki\n",
    "            cost_list_in_batch = []\n",
    "\n",
    "            for batch_index in range(0, n_samples, batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[batch_index:batch_index + batch_size]\n",
    "                y_batch = y_shuffled[batch_index:batch_index + batch_size].reshape(-1, 1)\n",
    "                predictions = self.forward(X_batch)\n",
    "                output_error = self.cross_entropy_derivative(y_batch, predictions)\n",
    "                self.backward(output_error)\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            cost = self.cross_entropy(y_batch, predictions)\n",
    "            cost_list.append(np.mean(cost))\n",
    "\n",
    "            if epoch % (epochs // 4) == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "\n",
    "            #jeśli koszt jest mniejszy niż epsilon to stop\n",
    "            # if(i > 0 and abs(cost - self.cost_list[-1]) < self.epsilon ):\n",
    "                # break\n",
    "            # self.cost_list.append(cost)\n",
    "            # iteration_cost = np.mean(cost_list_in_batch)\n",
    "            # self.mean_cost_list.append(iteration_cost)\n",
    "            # self.epoch_list.append(i)\n",
    "        print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 13) (237,)\n",
      "Epoch: 0, Loss: 0.8260260462120032\n",
      "Epoch: 250, Loss: 0.3696522400110217\n",
      "Epoch: 500, Loss: 0.2836277138829489\n",
      "Epoch: 750, Loss: 0.3555214253241455\n",
      "Epoch: 999, Loss: 0.4243593937986521\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_norm, Y, test_size=0.2)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "mlp = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=[10, 10], num_of_outputs=1)\n",
    "mlp.fit(X_train, Y_train, epochs=1000, batch_size=40, learning_rate=0.002)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
