{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "X = heart_disease.data.features\n",
    "Y = heart_disease.data.targets\n",
    "\n",
    "Y = Y['num'].replace([1, 2, 3, 4], 1)\n",
    "\n",
    "X['num'] = Y\n",
    "\n",
    "# median = X['ca'].median()\n",
    "# X['ca'].fillna(median, inplace=True)\n",
    "# mode_category = X['thal'].mode()[0]\n",
    "# X['thal'].fillna(mode_category, inplace=True)\n",
    "\n",
    "X = X.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wypełnianie brakujących wartości W kolumnie 'ca' oraz 'thal' wartością najczęściej występującą w tej kolumnie, powoduje gorsze wyniki modelu domyślnego, dlatego postanowiono, że usuniemy te próbki z danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, column, column_names):\n",
    "    dummies = pd.get_dummies(df[column], prefix=column)\n",
    "    column_names = [column + '_' + str(name) for name in column_names]\n",
    "    dummies.columns = column_names\n",
    "    dummies = dummies.astype('int64')\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  one_hot_encode(X, 'cp', ['typical_angina', 'atypical_angina', 'non-anginal_pain', 'asymptomatic'])\n",
    "X = one_hot_encode(X, 'thal', ['normal', 'ST-T_wave_abnormality', 'left_ventricular_hypertrophy'])\n",
    "X = one_hot_encode(X, 'slope', ['upsloping', 'flat', 'downsloping'])\n",
    "X = one_hot_encode(X, 'restecg', ['normal', 'fixed_defect', 'reversable_defect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 0 to 301\n",
      "Data columns (total 23 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   age                                297 non-null    int64  \n",
      " 1   sex                                297 non-null    int64  \n",
      " 2   trestbps                           297 non-null    int64  \n",
      " 3   chol                               297 non-null    int64  \n",
      " 4   fbs                                297 non-null    int64  \n",
      " 5   thalach                            297 non-null    int64  \n",
      " 6   exang                              297 non-null    int64  \n",
      " 7   oldpeak                            297 non-null    float64\n",
      " 8   ca                                 297 non-null    float64\n",
      " 9   num                                297 non-null    int64  \n",
      " 10  cp_typical_angina                  297 non-null    int64  \n",
      " 11  cp_atypical_angina                 297 non-null    int64  \n",
      " 12  cp_non-anginal_pain                297 non-null    int64  \n",
      " 13  cp_asymptomatic                    297 non-null    int64  \n",
      " 14  thal_normal                        297 non-null    int64  \n",
      " 15  thal_ST-T_wave_abnormality         297 non-null    int64  \n",
      " 16  thal_left_ventricular_hypertrophy  297 non-null    int64  \n",
      " 17  slope_upsloping                    297 non-null    int64  \n",
      " 18  slope_flat                         297 non-null    int64  \n",
      " 19  slope_downsloping                  297 non-null    int64  \n",
      " 20  restecg_normal                     297 non-null    int64  \n",
      " 21  restecg_fixed_defect               297 non-null    int64  \n",
      " 22  restecg_reversable_defect          297 non-null    int64  \n",
      "dtypes: float64(2), int64(21)\n",
      "memory usage: 55.7 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target without missing values\n",
    "Y = X['num']\n",
    "X = X.drop('num', axis=1)\n",
    "\n",
    "# Normalize data to [0, 1] range\n",
    "X_norm = (X - X.min()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    def __init__(self, num_of_weights, stand_dev=1):\n",
    "        self.weights = np.random.normal(scale=stand_dev, size=num_of_weights)\n",
    "        self.bias = np.random.normal(scale=stand_dev)\n",
    "        self.X = None\n",
    "        self.derivative = None\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        # return np.exp(-np.logaddexp(0, -X))\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def sigmoid_derivative(self, X):\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.X = inputs\n",
    "        return self.sigmoid(np.dot(inputs, self.weights) + self.bias)\n",
    "\n",
    "    def backward(self, error, weights_next_layer=None):\n",
    "        if weights_next_layer is not None:\n",
    "            error = error.T @ weights_next_layer\n",
    "        self.derivative = error * self.sigmoid_derivative(np.dot(self.X,self.weights) + self.bias)\n",
    "        return self.derivative\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.weights -=  learning_rate * np.dot(self.X.T, self.derivative)\n",
    "        self.bias -= learning_rate * np.sum(self.derivative)\n",
    "        self.X = None\n",
    "        self.derivative = None\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, num_of_inputs, hidden_layers, num_of_outputs, stand_dev=1.0):\n",
    "        self.layers = []\n",
    "        self.num_of_inputs = num_of_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_of_outputs = num_of_outputs\n",
    "        self.num_of_hidden_layers = len(hidden_layers)\n",
    "        self.stand_dev = stand_dev\n",
    "        \n",
    "        self.layers.append([Neuron(num_of_inputs, self.stand_dev) for _ in range(hidden_layers[0])])\n",
    "        for i in range(1, self.num_of_hidden_layers):\n",
    "            self.layers.append([Neuron(hidden_layers[i-1], self.stand_dev) for _ in range(hidden_layers[i])])\n",
    "        if self.num_of_outputs == 1:\n",
    "            self.layers.append([Neuron(hidden_layers[-1], self.stand_dev)])\n",
    "\n",
    "    def cross_entropy(self, y, y_pred):\n",
    "        return -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred), axis=1)\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_pred):\n",
    "        return -np.sum(y / y_pred - (1 - y) / (1 - y_pred), axis=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = np.array([neuron.forward(X) for neuron in layer]).T\n",
    "        return X        \n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # Iterate over the hidden layers in reverse order and calculate their gradients\n",
    "        output_layer = self.layers[-1]\n",
    "        output_error = np.array([neuron.backward(output_error) for neuron in output_layer])\n",
    "        weights_next_layer = np.array([neuron.weights for neuron in output_layer]).T\n",
    "\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            output_error = np.array([neuron.backward(output_error, weights_next_layer[index]) for index, neuron in enumerate(layer)])\n",
    "            weights_next_layer = np.array([neuron.weights for neuron in layer]).T\n",
    "        \n",
    "        # Return the final gradients\n",
    "        return output_error\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer:\n",
    "                neuron.update(learning_rate)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, batch_size=10, learning_rate=0.005):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Jeśli batch_size jest większy niż liczba próbek, to ustawiamy go na liczbę próbek -> zwyczajnie uczymy na całym zbiorze GD\n",
    "        if batch_size > n_samples:\n",
    "            batch_size = n_samples\n",
    "        cost_list = []\n",
    "        for epoch in range(epochs):\n",
    "            random_order = np.random.permutation(n_samples)\n",
    "            X_shuffled = X.values[random_order]\n",
    "            y_shuffled = y.values[random_order]\n",
    "            # lista kosztów w paczkach dla każdej epoki\n",
    "            cost_list_in_batch = []\n",
    "\n",
    "            for batch_index in range(0, n_samples, batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[batch_index:batch_index + batch_size]\n",
    "                y_batch = y_shuffled[batch_index:batch_index + batch_size].reshape(-1, 1)\n",
    "                predictions = self.forward(X_batch)\n",
    "                output_error = self.cross_entropy_derivative(y_batch, predictions)\n",
    "                self.backward(output_error)\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            cost = self.cross_entropy(y_batch, predictions)\n",
    "            cost_list.append(np.mean(cost))\n",
    "\n",
    "            if epoch % (epochs // 8) == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "\n",
    "            #jeśli koszt jest mniejszy niż epsilon to stop\n",
    "            # if(i > 0 and abs(cost - self.cost_list[-1]) < self.epsilon ):\n",
    "                # break\n",
    "            # self.cost_list.append(cost)\n",
    "            # iteration_cost = np.mean(cost_list_in_batch)\n",
    "            # self.mean_cost_list.append(iteration_cost)\n",
    "            # self.epoch_list.append(i)\n",
    "        print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "        return self, cost_list\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return np.exp(-np.logaddexp(0, -X))\n",
    "\n",
    "def sigmoid_derivative(X):\n",
    "    return sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "def cross_entropy(y, y_pred):\n",
    "    return -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred), axis=1)\n",
    "\n",
    "def cross_entropy_derivative(y, y_pred):\n",
    "    return -np.sum(y / y_pred - (1 - y) / (1 - y_pred), axis=1)\n",
    "\n",
    "class Neuron():\n",
    "    def __init__(self, num_of_weights, activation_function, activation_derivative, stand_dev=1):\n",
    "        self.weights = np.random.normal(scale=stand_dev, size=num_of_weights)\n",
    "        self.bias = np.random.normal(scale=stand_dev)\n",
    "        self.X = None\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.X = inputs\n",
    "        return self.activation_function(np.dot(inputs, self.weights) + self.bias)\n",
    "\n",
    "    def backward(self, error, weights_next_layer=None):\n",
    "        if weights_next_layer is not None:\n",
    "            error = error.T @ weights_next_layer\n",
    "        self.derivative = error * self.activation_derivative(np.dot(self.X, self.weights) + self.bias)\n",
    "        return self.derivative\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.weights -= learning_rate * np.dot(self.X.T, self.derivative)\n",
    "        self.bias -= learning_rate * np.sum(self.derivative)\n",
    "        self.X = None\n",
    "        self.derivative = None\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, num_of_inputs, hidden_layers, num_of_outputs, activation_function = sigmoid, activation_derivative = sigmoid_derivative, loss_function = cross_entropy, loss_derivative = cross_entropy_derivative, stand_dev=1.0):\n",
    "        self.layers = []\n",
    "        self.num_of_inputs = num_of_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_of_outputs = num_of_outputs\n",
    "        self.num_of_hidden_layers = len(hidden_layers)\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_derivative = loss_derivative\n",
    "        self.stand_dev = stand_dev\n",
    "\n",
    "        self.layers.append([Neuron(num_of_inputs, activation_function, activation_derivative, self.stand_dev) for _ in range(hidden_layers[0])])\n",
    "        for i in range(1, self.num_of_hidden_layers):\n",
    "            self.layers.append([Neuron(hidden_layers[i-1], activation_function, activation_derivative, self.stand_dev) for _ in range(hidden_layers[i])])\n",
    "        if self.num_of_outputs == 1:\n",
    "            self.layers.append([Neuron(hidden_layers[-1], activation_function, activation_derivative, self.stand_dev)])\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = np.array([neuron.forward(X) for neuron in layer]).T\n",
    "        return X\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        output_layer = self.layers[-1]\n",
    "        output_error = np.array([neuron.backward(output_error) for neuron in output_layer])\n",
    "        weights_next_layer = np.array([neuron.weights for neuron in output_layer]).T\n",
    "\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            output_error = np.array([neuron.backward(output_error, weights_next_layer[index]) for index, neuron in enumerate(layer)])\n",
    "            weights_next_layer = np.array([neuron.weights for neuron in layer]).T\n",
    "\n",
    "        return output_error\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer:\n",
    "                neuron.update(learning_rate)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, batch_size=10, learning_rate=0.005):\n",
    "        n_samples, n_features = X.shape\n",
    "        if batch_size > n_samples:\n",
    "            batch_size = n_samples\n",
    "        cost_list = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            random_order = np.random.permutation(n_samples)\n",
    "            X_shuffled = X.values[random_order]\n",
    "            y_shuffled = y.values[random_order]\n",
    "\n",
    "            cost_list_in_batch = []\n",
    "\n",
    "            for batch_index in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[batch_index:batch_index + batch_size]\n",
    "                y_batch = y_shuffled[batch_index:batch_index + batch_size].reshape(-1, 1)\n",
    "                predictions = self.forward(X_batch)\n",
    "                output_error = self.loss_derivative(y_batch, predictions)\n",
    "                self.backward(output_error)\n",
    "                self.update(learning_rate)\n",
    "\n",
    "            cost = self.loss_function(y_batch, predictions)\n",
    "            cost_list.append(np.mean(cost))\n",
    "\n",
    "            if epoch % (epochs // 8) == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "\n",
    "        print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "        return self, cost_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasa Neuron reprezentuje pojedynczy neuron w sieci neuronowej. Przyjmuje następujące parametry podczas inicjalizacji:\n",
    "\n",
    "- num_of_weights: Liczba wag (parametrów) neuronu.\n",
    "- activation_function: Funkcja aktywacji neuronu.\n",
    "- activation_derivative: Pochodna funkcji aktywacji neuronu.\n",
    "- stand_dev: Odchylenie standardowe używane do inicjalizacji wag neuronu (domyślnie 1).\n",
    "Metody klasy Neuron obejmują:\n",
    "\n",
    "- forward(self, inputs): Wykonuje krok przód, obliczając wyjście neuronu na podstawie wejścia i funkcji aktywacji.\n",
    "- backward(self, error, weights_next_layer=None): Wykonuje krok wstecz, obliczając pochodną błędu względem wejścia neuronu. Jeśli - dostępne są wagi warstwy następnej, można je przekazać, aby obliczyć błąd wsteczny w oparciu o te wagi.\n",
    "- update(self, learning_rate): Aktualizuje wagi neuronu na podstawie pochodnej błędu i współczynnika uczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.8366587713720809\n",
      "Epoch: 125, Loss: 0.17555607988440508\n",
      "Epoch: 250, Loss: 0.155472601691947\n",
      "Epoch: 375, Loss: 0.19044257397183223\n",
      "Epoch: 500, Loss: 0.08761761573339792\n",
      "Epoch: 625, Loss: 0.26155733617304106\n",
      "Epoch: 750, Loss: 0.02290420071616795\n",
      "Epoch: 875, Loss: 0.030357829459238338\n",
      "Epoch: 999, Loss: 0.11920780047209564\n",
      "Accuracy: 0.85\n",
      "Precision: 0.8\n",
      "F1: 0.816326530612245\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_norm, Y, test_size=0.2)\n",
    "mlp = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=[8, 4], num_of_outputs=1)\n",
    "mlp.fit(X_train, Y_train, epochs=1000, batch_size=10, learning_rate=0.005)\n",
    "predictions = mlp.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "print(f'Accuracy: {accuracy_score(Y_test, predictions)}')\n",
    "print(f'Precision: {precision_score(Y_test, predictions)}')\n",
    "print(f'F1: {f1_score(Y_test, predictions)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6694471193319574\n",
      "Epoch: 125, Loss: 0.38844936451380113\n",
      "Epoch: 250, Loss: 0.26240670050851156\n",
      "Epoch: 375, Loss: 0.12076295494995304\n",
      "Epoch: 500, Loss: 0.04224397525743791\n",
      "Epoch: 625, Loss: 0.13869862523886195\n",
      "Epoch: 750, Loss: 0.2134002784342621\n",
      "Epoch: 875, Loss: 0.11552206226093573\n",
      "Epoch: 999, Loss: 0.5652591722247716\n",
      "Accuracy: 0.85\n",
      "Precision: 0.8\n",
      "F1: 0.816326530612245\n"
     ]
    }
   ],
   "source": [
    "dim_hidd_1 = [5, 5]\n",
    "dim_hidd_1_nn = NeuralNetwork(X_train.shape[1], [8,4], 1)\n",
    "trained, lossh =  dim_hidd_1_nn.fit(X_train, Y_train, epochs=1000, batch_size=10, learning_rate=0.005)\n",
    "\n",
    "predictions = dim_hidd_1_nn.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "print(f'Accuracy: {accuracy_score(Y_test, predictions)}')\n",
    "print(f'Precision: {precision_score(Y_test, predictions)}')\n",
    "print(f'F1: {f1_score(Y_test, predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default\n",
      "Epoch: 0, Loss: 0.7524962040592994\n",
      "Epoch: 125, Loss: 0.7009894242599631\n",
      "Epoch: 250, Loss: 0.6878027957694791\n",
      "Epoch: 375, Loss: 0.687521041446641\n",
      "Epoch: 500, Loss: 0.6872562733784356\n",
      "Epoch: 625, Loss: 0.6642521104997606\n",
      "Epoch: 750, Loss: 0.6887148005676039\n",
      "Epoch: 875, Loss: 0.6763290951050083\n",
      "Epoch: 999, Loss: 0.700457546494633\n",
      "----------------\n",
      "Dim hidd 1\n",
      "Epoch: 0, Loss: 0.7788148788470411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\AppData\\Local\\Temp\\ipykernel_16188\\2817502624.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss: 0.6884267189244628\n",
      "Epoch: 250, Loss: 0.7141863191867582\n",
      "Epoch: 375, Loss: 0.6865939279111491\n",
      "Epoch: 500, Loss: 0.6748181432260546\n",
      "Epoch: 625, Loss: 0.688133295998273\n",
      "Epoch: 750, Loss: 0.6795714939763995\n",
      "Epoch: 875, Loss: 0.7023511686459337\n",
      "Epoch: 999, Loss: 0.7029005102136424\n",
      "----------------\n",
      "Dim hidd 2\n",
      "Epoch: 0, Loss: 0.686054370092099\n",
      "Epoch: 125, Loss: 0.6940996139792415\n",
      "Epoch: 250, Loss: 0.7105935214175999\n",
      "Epoch: 375, Loss: 0.7087787605726472\n",
      "Epoch: 500, Loss: 0.6847954959929551\n",
      "Epoch: 625, Loss: 0.7033544615807016\n",
      "Epoch: 750, Loss: 0.7211842799646612\n",
      "Epoch: 875, Loss: 0.6512631052658333\n",
      "Epoch: 999, Loss: 0.7082211390154834\n",
      "----------------\n",
      "Dim hidd 3\n",
      "Epoch: 0, Loss: 0.6898126656276018\n",
      "Epoch: 125, Loss: 0.6554569368566225\n",
      "Epoch: 250, Loss: 0.7493743587541\n",
      "Epoch: 375, Loss: 0.6725021344334061\n",
      "Epoch: 500, Loss: 0.6637303372042327\n",
      "Epoch: 625, Loss: 0.8089405860988051\n",
      "Epoch: 750, Loss: 0.7283553390593047\n",
      "Epoch: 875, Loss: 0.6764177345470307\n",
      "Epoch: 999, Loss: 0.6664166882651702\n",
      "----------------\n",
      "Learning rate 1\n",
      "Epoch: 0, Loss: 0.8614882987207272\n",
      "Epoch: 125, Loss: 0.6519556612101035\n",
      "Epoch: 250, Loss: 0.6877757378626168\n",
      "Epoch: 375, Loss: 0.6741323635619423\n",
      "Epoch: 500, Loss: 0.7019049260699267\n",
      "Epoch: 625, Loss: 0.6756423560529206\n",
      "Epoch: 750, Loss: 0.6559893590508601\n",
      "Epoch: 875, Loss: 0.7011682767213375\n",
      "Epoch: 999, Loss: 0.6748628451529776\n",
      "----------------\n",
      "Learning rate 2\n",
      "Epoch: 0, Loss: 0.6724659705832375\n",
      "Epoch: 125, Loss: 0.69275869797622\n",
      "Epoch: 250, Loss: 0.688187167651363\n",
      "Epoch: 375, Loss: 0.6828795818831821\n",
      "Epoch: 500, Loss: 0.6879862110584919\n",
      "Epoch: 625, Loss: 0.6830735339481572\n",
      "Epoch: 750, Loss: 0.6979398373868728\n",
      "Epoch: 875, Loss: 0.7147644174526401\n",
      "Epoch: 999, Loss: 0.6862740724802084\n",
      "----------------\n",
      "Stand dev 1\n",
      "Epoch: 0, Loss: 0.69014587987664\n",
      "Epoch: 125, Loss: 0.7116591503157679\n",
      "Epoch: 250, Loss: 0.7025346550623132\n",
      "Epoch: 375, Loss: 0.6807214953919419\n",
      "Epoch: 500, Loss: 0.6758142972762693\n",
      "Epoch: 625, Loss: 0.6870179705588532\n",
      "Epoch: 750, Loss: 0.7073576202981878\n",
      "Epoch: 875, Loss: 0.6654826319344805\n",
      "Epoch: 999, Loss: 0.6713009004624125\n",
      "----------------\n",
      "Stand dev 2\n",
      "Epoch: 0, Loss: 6.76387078632083\n",
      "Epoch: 125, Loss: 0.6859827253004135\n",
      "Epoch: 250, Loss: 0.742727837966494\n",
      "Epoch: 375, Loss: 0.7013961162090132\n",
      "Epoch: 500, Loss: 0.7041081035206416\n",
      "Epoch: 625, Loss: 0.6969592513818067\n",
      "Epoch: 750, Loss: 0.7013706526636875\n",
      "Epoch: 875, Loss: 0.6879166170660065\n",
      "Epoch: 999, Loss: 0.7347238806706284\n",
      "----------------\n",
      "Unnorm\n",
      "Epoch: 0, Loss: 0.794152514921673\n",
      "Epoch: 125, Loss: 0.4585454058521625\n",
      "Epoch: 250, Loss: 0.14861878797855382\n",
      "Epoch: 375, Loss: 0.06820262047205405\n",
      "Epoch: 500, Loss: 0.0950887933722677\n",
      "Epoch: 625, Loss: 0.3836959116629338\n",
      "Epoch: 750, Loss: 0.31474234676594287\n",
      "Epoch: 875, Loss: 0.2271038738924255\n",
      "Epoch: 999, Loss: 0.1735665362261485\n",
      "----------------\n",
      "Hidden layers size 1\n",
      "Epoch: 0, Loss: 0.6855455028306137\n",
      "Epoch: 125, Loss: 0.6773249685989754\n",
      "Epoch: 250, Loss: 0.7011691558778487\n",
      "Epoch: 375, Loss: 0.6412778418797885\n",
      "Epoch: 500, Loss: 0.6536499939079153\n",
      "Epoch: 625, Loss: 0.7047545250111702\n",
      "Epoch: 750, Loss: 0.6844449260739803\n",
      "Epoch: 875, Loss: 0.6630000020062268\n",
      "Epoch: 999, Loss: 0.6179178323291689\n",
      "----------------\n",
      "Hidden layers size 2\n",
      "Epoch: 0, Loss: 0.9087899554693627\n",
      "Epoch: 125, Loss: 0.728839058167524\n",
      "Epoch: 250, Loss: 0.7472598977565905\n",
      "Epoch: 375, Loss: 0.7039457584172383\n",
      "Epoch: 500, Loss: 0.6250526609682056\n",
      "Epoch: 625, Loss: 0.7092297132214302\n",
      "Epoch: 750, Loss: 0.6960835784816123\n",
      "Epoch: 875, Loss: 0.7476107128520905\n",
      "Epoch: 999, Loss: 0.6717036578074903\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "X_train_n, X_test_n, Y_train_n, Y_test_n = train_test_split(X_norm, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# domyślna sieć\n",
    "default_hidden_layers = [10, 5]\n",
    "\n",
    "#1.Różnej wymiarowości warstwy ukrytej\n",
    "dim_hidd_1 = [5, 5]\n",
    "dim_hidd_2 = [20, 10]\n",
    "dim_hidd_3 = [50, 25]\n",
    "#2. Różnej wartości współczynnika uczenia\n",
    "learning_rate_1 = 0.001\n",
    "learning_rate_2 = 0.01\n",
    "#3. Różnej wartości parametru standaryzacji\n",
    "stand_dev_1 = 0.1\n",
    "stand_dev_2 = 5\n",
    "#4.danych znormalizownaych i nieznormalizowanych\n",
    "unnorm = X_train, Y_train\n",
    "#5. Różnej liczby watstw ukrytych\n",
    "hidden_layers_size_1 = [10]\n",
    "hidden_layers_size_2 = [10,5,2]\n",
    "hidden_layers_size_3 = [10,5,3,2]\n",
    "\n",
    "default_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "dim_hidd_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=dim_hidd_1, num_of_outputs=1)\n",
    "dim_hidd_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=dim_hidd_2, num_of_outputs=1)\n",
    "dim_hidd_3_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=dim_hidd_3, num_of_outputs=1)\n",
    "learning_rate_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "learning_rate_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "stand_dev_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1, stand_dev=stand_dev_1)\n",
    "stand_dev_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1, stand_dev=stand_dev_2)\n",
    "unnorm_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "hidden_layers_size_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=hidden_layers_size_1, num_of_outputs=1)\n",
    "hidden_layers_size_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=hidden_layers_size_2, num_of_outputs=1)\n",
    "\n",
    "print('Default')\n",
    "default_mlp, loss_history_1 = default_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Dim hidd 1')\n",
    "dim_hidd_1_mlp, loss_history_2 = dim_hidd_1_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Dim hidd 2')\n",
    "dim_hidd_2_mlp, loss_history_3 = dim_hidd_2_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Dim hidd 3')\n",
    "dim_hidd_3_mlp, loss_history_4 = dim_hidd_3_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Learning rate 1')\n",
    "learning_rate_1_mlp, loss_history_5 = learning_rate_1_nn.fit(X_train, Y_train, learning_rate=learning_rate_1)\n",
    "print('----------------')\n",
    "print('Learning rate 2')\n",
    "learning_rate_2_mlp, loss_history_6 = learning_rate_2_nn.fit(X_train, Y_train, learning_rate=learning_rate_2)\n",
    "print('----------------')\n",
    "print('Stand dev 1')\n",
    "stand_dev_1_mlp, loss_history_7 = stand_dev_1_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Stand dev 2')\n",
    "stand_dev_2_mlp, loss_history_8 = stand_dev_2_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Unnorm')\n",
    "unnorm_mlp, loss_history_9 = unnorm_nn.fit(X_train_n, Y_train_n)\n",
    "print('----------------')\n",
    "print('Hidden layers size 1')\n",
    "hidden_layers_size_1_mlp, loss_history_10 = hidden_layers_size_1_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Hidden layers size 2')\n",
    "hidden_layers_size_2_mlp, loss_history_11 = hidden_layers_size_2_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "Precision: 0.0\n",
      "F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = default_mlp.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "print(f'Accuracy: {accuracy_score(Y_test, predictions)}')\n",
    "print(f'Precision: {precision_score(Y_test, predictions)}')\n",
    "print(f'F1: {f1_score(Y_test, predictions)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
