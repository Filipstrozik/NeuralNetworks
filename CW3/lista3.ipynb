{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "pd.set_option('display.width', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "X = heart_disease.data.features\n",
    "Y = heart_disease.data.targets\n",
    "\n",
    "# Convert target to binary\n",
    "Y = Y['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Join features and target\n",
    "X['target'] = Y\n",
    "# Remove records with missing values\n",
    "X = X[~np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, column, column_names):\n",
    "    dummies = pd.get_dummies(df[column], prefix=column)\n",
    "    column_names = [column + '_' + str(name) for name in column_names]\n",
    "    dummies.columns = column_names\n",
    "    dummies = dummies.astype('int64')\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  one_hot_encode(X, 'cp', ['typical_angina', 'atypical_angina', 'non-anginal_pain', 'asymptomatic'])\n",
    "X = one_hot_encode(X, 'thal', ['normal', 'ST-T_wave_abnormality', 'left_ventricular_hypertrophy'])\n",
    "X = one_hot_encode(X, 'slope', ['upsloping', 'flat', 'downsloping'])\n",
    "X = one_hot_encode(X, 'restecg', ['normal', 'fixed_defect', 'reversable_defect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 297 entries, 0 to 301\n",
      "Data columns (total 23 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   age                                297 non-null    int64  \n",
      " 1   sex                                297 non-null    int64  \n",
      " 2   trestbps                           297 non-null    int64  \n",
      " 3   chol                               297 non-null    int64  \n",
      " 4   fbs                                297 non-null    int64  \n",
      " 5   thalach                            297 non-null    int64  \n",
      " 6   exang                              297 non-null    int64  \n",
      " 7   oldpeak                            297 non-null    float64\n",
      " 8   ca                                 297 non-null    float64\n",
      " 9   target                             297 non-null    int64  \n",
      " 10  cp_typical_angina                  297 non-null    int64  \n",
      " 11  cp_atypical_angina                 297 non-null    int64  \n",
      " 12  cp_non-anginal_pain                297 non-null    int64  \n",
      " 13  cp_asymptomatic                    297 non-null    int64  \n",
      " 14  thal_normal                        297 non-null    int64  \n",
      " 15  thal_ST-T_wave_abnormality         297 non-null    int64  \n",
      " 16  thal_left_ventricular_hypertrophy  297 non-null    int64  \n",
      " 17  slope_upsloping                    297 non-null    int64  \n",
      " 18  slope_flat                         297 non-null    int64  \n",
      " 19  slope_downsloping                  297 non-null    int64  \n",
      " 20  restecg_normal                     297 non-null    int64  \n",
      " 21  restecg_fixed_defect               297 non-null    int64  \n",
      " 22  restecg_reversable_defect          297 non-null    int64  \n",
      "dtypes: float64(2), int64(21)\n",
      "memory usage: 55.7 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target without missing values\n",
    "Y = X['target']\n",
    "X = X.drop('target', axis=1)\n",
    "\n",
    "# Normalize data to [0, 1] range\n",
    "X_norm = (X - X.min()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron():\n",
    "    def __init__(self, num_of_weights, stand_dev=1):\n",
    "        self.weights = np.random.normal(scale=stand_dev, size=num_of_weights)\n",
    "        self.bias = np.random.normal(scale=stand_dev)\n",
    "        self.X = None\n",
    "        self.derivative = None\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return np.exp(-np.logaddexp(0, -X))\n",
    "\n",
    "    def sigmoid_derivative(self, X):\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.X = inputs\n",
    "        return self.sigmoid(np.dot(inputs, self.weights) + self.bias)\n",
    "\n",
    "    def backward(self, error, weights_next_layer=None):\n",
    "        if weights_next_layer is not None:\n",
    "            error = error.T @ weights_next_layer\n",
    "        self.derivative = error * self.sigmoid_derivative(np.dot(self.X,self.weights) + self.bias)\n",
    "        return self.derivative\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.weights -=  learning_rate * np.dot(self.X.T, self.derivative)\n",
    "        self.bias -= learning_rate * np.sum(self.derivative)\n",
    "        self.X = None\n",
    "        self.derivative = None\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, num_of_inputs, hidden_layers, num_of_outputs, stand_dev=1.0):\n",
    "        self.layers = []\n",
    "        self.num_of_inputs = num_of_inputs\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.num_of_outputs = num_of_outputs\n",
    "        self.num_of_hidden_layers = len(hidden_layers)\n",
    "        self.stand_dev = stand_dev\n",
    "        \n",
    "        self.layers.append([Neuron(num_of_inputs, self.stand_dev) for _ in range(hidden_layers[0])])\n",
    "        for i in range(1, self.num_of_hidden_layers):\n",
    "            self.layers.append([Neuron(hidden_layers[i-1], self.stand_dev) for _ in range(hidden_layers[i])])\n",
    "            if self.num_of_outputs == 1:\n",
    "                self.layers.append([Neuron(hidden_layers[-1], self.stand_dev)])\n",
    "\n",
    "    def cross_entropy(self, y, y_pred):\n",
    "        return -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred), axis=1)\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_pred):\n",
    "        return -np.sum(y / y_pred - (1 - y) / (1 - y_pred), axis=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = np.array([neuron.forward(X) for neuron in layer]).T\n",
    "        return X        \n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # Iterate over the hidden layers in reverse order and calculate their gradients\n",
    "        output_layer = self.layers[-1]\n",
    "        output_error = np.array([neuron.backward(output_error) for neuron in output_layer])\n",
    "        weights_next_layer = np.array([neuron.weights for neuron in output_layer]).T\n",
    "\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            output_error = np.array([neuron.backward(output_error, weights_next_layer[index]) for index, neuron in enumerate(layer)])\n",
    "            weights_next_layer = np.array([neuron.weights for neuron in layer]).T\n",
    "        \n",
    "        # Return the final gradients\n",
    "        return output_error\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer:\n",
    "                neuron.update(learning_rate)\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, batch_size=10, learning_rate=0.005):\n",
    "        n_samples, n_features = X.shape\n",
    "        # Jeśli batch_size jest większy niż liczba próbek, to ustawiamy go na liczbę próbek -> zwyczajnie uczymy na całym zbiorze GD\n",
    "        if batch_size > n_samples:\n",
    "            batch_size = n_samples\n",
    "        cost_list = []\n",
    "        for epoch in range(epochs):\n",
    "            random_order = np.random.permutation(n_samples)\n",
    "            X_shuffled = X.values[random_order]\n",
    "            y_shuffled = y.values[random_order]\n",
    "            # lista kosztów w paczkach dla każdej epoki\n",
    "            cost_list_in_batch = []\n",
    "\n",
    "            for batch_index in range(0, n_samples, batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[batch_index:batch_index + batch_size]\n",
    "                y_batch = y_shuffled[batch_index:batch_index + batch_size].reshape(-1, 1)\n",
    "                predictions = self.forward(X_batch)\n",
    "                output_error = self.cross_entropy_derivative(y_batch, predictions)\n",
    "                self.backward(output_error)\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "            cost = self.cross_entropy(y_batch, predictions)\n",
    "            cost_list.append(np.mean(cost))\n",
    "\n",
    "            if epoch % (epochs // 4) == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "\n",
    "            #jeśli koszt jest mniejszy niż epsilon to stop\n",
    "            # if(i > 0 and abs(cost - self.cost_list[-1]) < self.epsilon ):\n",
    "                # break\n",
    "            # self.cost_list.append(cost)\n",
    "            # iteration_cost = np.mean(cost_list_in_batch)\n",
    "            # self.mean_cost_list.append(iteration_cost)\n",
    "            # self.epoch_list.append(i)\n",
    "        print(f'Epoch: {epoch}, Loss: {np.mean(cost)}')\n",
    "        return self, cost_list\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 22) (237,)\n",
      "Epoch: 0, Loss: 0.8366587713720809\n",
      "Epoch: 250, Loss: 0.155472601691947\n",
      "Epoch: 500, Loss: 0.08761761573339792\n",
      "Epoch: 750, Loss: 0.02290420071616795\n",
      "Epoch: 999, Loss: 0.11920780047209564\n",
      "Accuracy: 0.85\n",
      "Precision: 0.8\n",
      "F1: 0.816326530612245\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_norm, Y, test_size=0.2)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "mlp = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=[8, 4], num_of_outputs=1)\n",
    "mlp.fit(X_train, Y_train, epochs=1000, batch_size=10, learning_rate=0.005)\n",
    "predictions = mlp.predict(X_test)\n",
    "predictions = np.where(predictions > 0.5, 1, 0)\n",
    "print(f'Accuracy: {accuracy_score(Y_test, predictions)}')\n",
    "print(f'Precision: {precision_score(Y_test, predictions)}')\n",
    "print(f'F1: {f1_score(Y_test, predictions)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default\n",
      "Epoch: 0, Loss: 0.6762192004566269\n",
      "Epoch: 250, Loss: 0.6897953892804561\n",
      "Epoch: 500, Loss: 0.6886138890573463\n",
      "Epoch: 750, Loss: 0.6885343321949521\n",
      "Epoch: 999, Loss: 0.702557653216698\n",
      "----------------\n",
      "Dim hidd 1\n",
      "Epoch: 0, Loss: 0.7197427636877303\n",
      "Epoch: 250, Loss: 0.6995024081987523\n",
      "Epoch: 500, Loss: 0.7022411284100204\n",
      "Epoch: 750, Loss: 0.7075719283221201\n",
      "Epoch: 999, Loss: 0.6895734189008669\n",
      "----------------\n",
      "Dim hidd 2\n",
      "Epoch: 0, Loss: 1.3567742223631023\n",
      "Epoch: 250, Loss: 0.7014702414086421\n",
      "Epoch: 500, Loss: 0.6902228095961532\n",
      "Epoch: 750, Loss: 0.6714796688097558\n",
      "Epoch: 999, Loss: 0.6895360226781305\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "X_train_n, X_test_n, Y_train_n, Y_test_n = train_test_split(X_norm, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# domyslnie \n",
    "default_hidden_layers = [10, 5]\n",
    "\n",
    "#1.Różnej wymiarowości warstwy ukrytej\n",
    "dim_hidd_1 = [5, 5]\n",
    "dim_hidd_2 = [20, 10]\n",
    "dim_hidd_3 = [50, 25]\n",
    "#2. Różnej wartości współczynnika uczenia\n",
    "learning_rate_1 = 0.001\n",
    "learning_rate_2 = 0.01\n",
    "#3. Różnej wartości parametru standaryzacji\n",
    "stand_dev_1 = 0.1\n",
    "stand_dev_2 = 5\n",
    "#4.danych znormalizownaych i nieznormalizowanych\n",
    "unnorm = X_train, Y_train\n",
    "#5. Różnej liczby watstw ukrytych\n",
    "hidden_layers_size_1 = [10]\n",
    "hidden_layers_size_2 = [10,5,2]\n",
    "hidden_layers_size_3 = [10,5,3,2]\n",
    "\n",
    "default_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "dim_hidd_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=dim_hidd_1, num_of_outputs=1)\n",
    "dim_hidd_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=dim_hidd_2, num_of_outputs=1)\n",
    "dim_hidd_3_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=dim_hidd_3, num_of_outputs=1)\n",
    "learning_rate_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "learning_rate_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "stand_dev_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "stand_dev_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "unnorm_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=default_hidden_layers, num_of_outputs=1)\n",
    "hidden_layers_size_1_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=hidden_layers_size_1, num_of_outputs=1)\n",
    "hidden_layers_size_2_nn = NeuralNetwork(num_of_inputs=X_train.shape[1], hidden_layers=hidden_layers_size_2, num_of_outputs=1)\n",
    "\n",
    "print('Default')\n",
    "default_mlp, loss_history_1 = default_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Dim hidd 1')\n",
    "dim_hidd_1_mlp, loss_history_2 = dim_hidd_1_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "print('Dim hidd 2')\n",
    "dim_hidd_2_mlp, loss_history_3 = dim_hidd_2_nn.fit(X_train, Y_train)\n",
    "print('----------------')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
