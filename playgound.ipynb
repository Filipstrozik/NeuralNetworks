{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class MiniBatchLogisticRegressionNetwork:\n",
    "    def __init__(self, layers, lr=0.001, epochs=1000, batch_size=5, seed=42, epsilon=0.0001):\n",
    "        self.layers = layers\n",
    "        self.lr = lr # współczynnik uczenia\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.epsilon = epsilon\n",
    "        self.cost_list = []\n",
    "        self.mean_cost_list = []\n",
    "        self.epoch_list = []\n",
    "        self.weighted_sums = list()\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(0, len(layers)-1) ]\n",
    "        for i in self.weights:\n",
    "            print(i.shape)\n",
    "        self.biases = [np.zeros((1,i)) for i in layers[1:]]\n",
    "        for i in self.biases:\n",
    "            print(i.shape)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return -(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    \n",
    "    def derivative_sigmoid(self, x):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # print('forward')\n",
    "        self.weighted_sums = list()\n",
    "        a = X\n",
    "        for i in range(0, len(layers)-1):\n",
    "            # print('i', i)\n",
    "            # print('a', a.shape)\n",
    "            # print('self.weights[i]', self.weights[i].shape)\n",
    "            # print('dot.size',np.dot(a, self.weights[i]).shape)\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            print('z', z.shape)\n",
    "            a = sigmoid(z)\n",
    "            self.weighted_sums.append(z)\n",
    "        # for i in self.weighted_sums:\n",
    "        #     print(i.shape)\n",
    "        return a \n",
    "    \n",
    "    def backward(self, X_batch, y_batch, predicted):\n",
    "        # print('predicted', predicted.shape)\n",
    "        # print('y_batch', y_batch.shape)\n",
    "        loss_after = y_batch - predicted #after\n",
    "        #inaczej trzeba liczyć loss after - przy ostatniej warstwie jest ok ale niekoniecznie dla głębszych\n",
    "\n",
    "        # go from last layer to first layer\n",
    "        for i in range(len(layers)-2, 0, -1):\n",
    "\n",
    "            # print('i', i)\n",
    "            if( i-1 <= 0):\n",
    "                weighted_sum_before_before = X_batch\n",
    "            else:\n",
    "                weighted_sum_before_before = self.weighted_sums[i-2]\n",
    "                # weighted_sum_before = self.weighted_sums[i-1]\n",
    "            \n",
    "            # print('self.weighted_sums[i]', self.weighted_sums[i].shape)\n",
    "            # print('self.weighted_sums[i-1]', self.weighted_sums[i-1].shape)\n",
    "            # print('self.weights[i]', self.weights[i].shape)\n",
    "            # print('self.weights[i-1]', self.weights[i-1].shape)\n",
    "            # print('loss_after', loss_after.shape)\n",
    "            loss_before = self.derivative_sigmoid(self.weighted_sums[i-1]) * (np.dot(loss_after, self.weights[i].T))\n",
    "            # print('loss_before', loss_before.shape)\n",
    "            delta_next = self.lr * np.dot(self.weighted_sums[i-1].T, loss_after)\n",
    "            # print('delta_next', delta_next.shape)\n",
    "            self.weights[i] += delta_next\n",
    "            # print('self.weighted_sums[i-1]',self.weighted_sums[i-1].shape)\n",
    "            # print('X_batch',X_batch.shape)\n",
    "            delta_before = self.lr * np.dot(weighted_sum_before_before.T, loss_before)\n",
    "            # print('delta_before', delta_before.shape)\n",
    "\n",
    "            self.weights[i-1] += delta_before\n",
    "\n",
    "            loss_after = loss_before\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Jeśli batch_size jest większy niż liczba próbek, to ustawiamy go na liczbę próbek -> zwyczajnie uczymy na całym zbiorze GD\n",
    "        if self.batch_size > n_samples:\n",
    "            self.batch_size = n_samples\n",
    "\n",
    "        #inicjacja list na koszt i epoki\n",
    "        self.cost_list = []\n",
    "        self.mean_cost_list = []\n",
    "        self.epoch_list = []\n",
    "        cost = 0\n",
    "        #ustawiamy seed\n",
    "        np.random.seed(self.seed)\n",
    "        #dla każdej iteracji/epoki\n",
    "        for i in range(self.epochs):\n",
    "            print('epochs', i)\n",
    "            #tasowanie indeksów ale deterministycznie bo ustawiliśmy seed\n",
    "            random_order = np.random.permutation(n_samples)\n",
    "            #tasujemy X i y\n",
    "            X_shuffled = X[random_order]\n",
    "            y_shuffled = y[random_order]\n",
    "            # lista kosztów w paczkach dla każdej epoki\n",
    "            cost_list_in_batch = []\n",
    "\n",
    "            #dla każdej paczki\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[j:j + self.batch_size]\n",
    "                y_batch = y_shuffled[j:j + self.batch_size]\n",
    "                # print('X_batch', X_batch.shape)\n",
    "                # print('y_batch', y_batch.shape)\n",
    "\n",
    "                predictions = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch.reshape(y_batch.shape[0], 1), predictions)\n",
    "                # print ('predictions', predictions.shape)\n",
    "                \n",
    "                cost = np.mean(self.loss(y_batch, predictions))\n",
    "                cost_list_in_batch.append(cost)\n",
    "\n",
    "            #jeśli koszt jest mniejszy niż epsilon to stop\n",
    "            if(i > 0 and abs(cost - self.cost_list[-1]) < self.epsilon ):\n",
    "                break\n",
    "            self.cost_list.append(cost)\n",
    "            iteration_cost = np.mean(cost_list_in_batch)\n",
    "            self.mean_cost_list.append(iteration_cost)\n",
    "            self.epoch_list.append(i)\n",
    "        print('cost', cost, 'cost_list[-1]', self.cost_list[-1], 'i', i, 'abs(cost - cost_list[-1])', abs(cost - self.cost_list[-1]))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        linear_pred = self.forward(X)\n",
    "        class_pred = [0 if y <= 0.5 else 1 for y in linear_pred]\n",
    "        return class_pred\n",
    "        \n",
    "\n",
    "layers = [22, 10, 5, 1]\n",
    "clf = MiniBatchLogisticRegressionNetwork(layers, lr=0.001, epochs=1, batch_size=7, seed=27, epsilon=0.0001)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [20 21 22 23 24 25 26 27 28 29]\n",
      " [30 31 32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47 48 49]\n",
      " [50 51 52 53 54 55 56 57 58 59]\n",
      " [60 61 62 63 64 65 66 67 68 69]]\n",
      "[[0 1 2 3 4 5 6 7 8 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18],\n",
       "       [10, 12, 14, 16, 18, 20, 22, 24, 26, 28],\n",
       "       [20, 22, 24, 26, 28, 30, 32, 34, 36, 38],\n",
       "       [30, 32, 34, 36, 38, 40, 42, 44, 46, 48],\n",
       "       [40, 42, 44, 46, 48, 50, 52, 54, 56, 58],\n",
       "       [50, 52, 54, 56, 58, 60, 62, 64, 66, 68],\n",
       "       [60, 62, 64, 66, 68, 70, 72, 74, 76, 78]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "array1 = np.arange(70).reshape(7, 10)  # Shape (7, 10)\n",
    "array2 = np.array([np.arange(10)])    # Shape (1, 10)\n",
    "\n",
    "print(array1)\n",
    "print(array2)\n",
    "\n",
    "# Perform the addition\n",
    "result = array1 + array2\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class MiniBatchLogisticRegressionNetwork:\n",
    "    def __init__(self, layers, lr=0.001, epochs=1000, batch_size=5, seed=42, epsilon=0.0001):\n",
    "        self.layers = layers\n",
    "        self.lr = lr # współczynnik uczenia\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = seed\n",
    "        self.epsilon = epsilon\n",
    "        self.cost_list = []\n",
    "        self.mean_cost_list = []\n",
    "        self.epoch_list = []\n",
    "        self.weighted_sums = list()\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(0, len(layers)-1) ]\n",
    "        for i in self.weights:\n",
    "            print(i.shape)\n",
    "        self.biases = [np.zeros((1,i)) for i in layers[1:]]\n",
    "        for i in self.biases:\n",
    "            print(i.shape)\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return -(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    \n",
    "    def derivative_sigmoid(self, x):\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # print('forward')\n",
    "        self.weighted_sums = list()\n",
    "        a = X\n",
    "        for i in range(0, len(layers)-1):\n",
    "            # print('i', i)\n",
    "            print('a', a.shape)\n",
    "            print('self.weights[i]', self.weights[i].shape)\n",
    "            print('self.biases[i]', self.biases[i].shape)\n",
    "            print('dot.size',np.dot(a, self.weights[i]).shape)\n",
    "            z = np.dot(a, self.weights[i]) + self.biases[i]\n",
    "            print('z', z.shape)\n",
    "            a = sigmoid(z)\n",
    "            self.weighted_sums.append(z)\n",
    "        # for i in self.weighted_sums:\n",
    "        #     print(i.shape)\n",
    "        return a \n",
    "    \n",
    "    def backward(self, X_batch, y_batch, predicted):\n",
    "        # print('predicted', predicted.shape)\n",
    "        # print('y_batch', y_batch.shape)\n",
    "        loss_after = y_batch - predicted #after\n",
    "        #inaczej trzeba liczyć loss after - przy ostatniej warstwie jest ok ale niekoniecznie dla głębszych\n",
    "        loss_before_history = list()\n",
    "        # go from last layer to first layer\n",
    "        for i in range(len(layers)-2, -1, -1):\n",
    "\n",
    "            print('i', i)\n",
    "            if( i <= 0):\n",
    "                a_before = X_batch\n",
    "            else:\n",
    "                a_before = self.weighted_sums[i-1]\n",
    "                # weighted_sum_before = self.weighted_sums[i-1]\n",
    "            # print('self.weighted_sums[i]', self.weighted_sums[i].shape)\n",
    "            # print('self.weighted_sums[i-1]', self.weighted_sums[i-1].shape)\n",
    "            # print('a_before', a_before.shape)\n",
    "\n",
    "            # print('self.weights[i]', self.weights[i].shape)\n",
    "            # print('self.weights[i-1]', self.weights[i-1].shape)\n",
    "            # print('loss_after', loss_after.shape)\n",
    "            if(i == len(layers)-2):\n",
    "                loss_before = np.dot(loss_after, self.weights[i].T)\n",
    "            else:\n",
    "                loss_before = self.derivative_sigmoid(a_before) * (np.dot(loss_after, self.weights[i].T))\n",
    "            loss_before_history.append(loss_before)\n",
    "                \n",
    "\n",
    "            # delta_next = self.lr * np.dot(a_before.T, loss_after)\n",
    "            # print('delta_next', delta_next.shape)\n",
    "            # print('self.weights[i]', self.weights[i])\n",
    "            # self.weights[i] += delta_next\n",
    "            # print('self.weights[i]', self.weights[i])\n",
    "            # print('self.weighted_sums[i-1]',self.weighted_sums[i-1].shape)\n",
    "            # print('X_batch',X_batch.shape)\n",
    "\n",
    "            # Tutaj mam wrażenie, że raz zmieniam wagi a potem zmieniam je jeszcze raz \n",
    "\n",
    "\n",
    "            # delta_before = self.lr * np.dot(weighted_sum_before_before.T, loss_before)\n",
    "            # print('self.weights[i-1]', self.weights[i-1])\n",
    "            # self.weights[i-1] += delta_before\n",
    "            # print('self.weights[i-1]', self.weights[i-1])\n",
    "\n",
    "\n",
    "            #dodaj biasy.\n",
    "\n",
    "            loss_after = loss_before\n",
    "        index = 0\n",
    "        for i in reversed(loss_before_history):\n",
    "            delta_next = self.lr * np.dot(i.T, self.weighted_sums[index])\n",
    "            self.weights[index] += delta_next\n",
    "            \n",
    "            delta_next_b = self.lr * np.sum(i, axis=1, keepdims=True)\n",
    "            print('self.biases[index]', self.biases[index].shape)\n",
    "            print('delta_next_b', delta_next_b.shape)\n",
    "            self.biases[index] += delta_next_b\n",
    "            index += 1\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Jeśli batch_size jest większy niż liczba próbek, to ustawiamy go na liczbę próbek -> zwyczajnie uczymy na całym zbiorze GD\n",
    "        if self.batch_size > n_samples:\n",
    "            self.batch_size = n_samples\n",
    "\n",
    "        #inicjacja list na koszt i epoki\n",
    "        self.cost_list = []\n",
    "        self.mean_cost_list = []\n",
    "        self.epoch_list = []\n",
    "        cost = 0\n",
    "        #ustawiamy seed\n",
    "        np.random.seed(self.seed)\n",
    "        #dla każdej iteracji/epoki\n",
    "        for i in range(self.epochs):\n",
    "            print('epochs', i)\n",
    "            #tasowanie indeksów ale deterministycznie bo ustawiliśmy seed\n",
    "            random_order = np.random.permutation(n_samples)\n",
    "            #tasujemy X i y\n",
    "            X_shuffled = X[random_order]\n",
    "            y_shuffled = y[random_order]\n",
    "            # lista kosztów w paczkach dla każdej epoki\n",
    "            cost_list_in_batch = []\n",
    "\n",
    "            #dla każdej paczki\n",
    "            for j in range(0, n_samples, self.batch_size):\n",
    "\n",
    "                X_batch = X_shuffled[j:j + self.batch_size]\n",
    "                y_batch = y_shuffled[j:j + self.batch_size]\n",
    "                # print('X_batch', X_batch.shape)\n",
    "                # print('y_batch', y_batch.shape)\n",
    "\n",
    "                predictions = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch.reshape(y_batch.shape[0], 1), predictions)\n",
    "                # print ('predictions', predictions.shape)\n",
    "                \n",
    "                cost = np.mean(self.loss(y_batch, predictions))\n",
    "                cost_list_in_batch.append(cost)\n",
    "\n",
    "            #jeśli koszt jest mniejszy niż epsilon to stop\n",
    "            if(i > 0 and abs(cost - self.cost_list[-1]) < self.epsilon ):\n",
    "                break\n",
    "            self.cost_list.append(cost)\n",
    "            iteration_cost = np.mean(cost_list_in_batch)\n",
    "            self.mean_cost_list.append(iteration_cost)\n",
    "            self.epoch_list.append(i)\n",
    "        print('cost', cost, 'cost_list[-1]', self.cost_list[-1], 'i', i, 'abs(cost - cost_list[-1])', abs(cost - self.cost_list[-1]))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        linear_pred = self.forward(X)\n",
    "        class_pred = [0 if y <= 0.5 else 1 for y in linear_pred]\n",
    "        return class_pred\n",
    "        \n",
    "\n",
    "layers = [22, 10,5, 1]\n",
    "clf = MiniBatchLogisticRegressionNetwork(layers, lr=0.001, epochs=1, batch_size=7, seed=27, epsilon=0.0001)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.268"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  = 3.134\n",
    "a*2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
